{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f51905",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a811783",
   "metadata": {},
   "source": [
    "Github repo for assignment: https://github.com/brentonjackson/csc-4980/tree/master/Assignment4\n",
    "\n",
    "I'll be using Python for the assignments in this class, as opposed to Matlab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3812e",
   "metadata": {},
   "source": [
    "## Part I\n",
    "\n",
    "Implement an application using the stereo camera where it will recognize, track and\n",
    "estimate dimensions of an object within 3m distance and inside field-of-view to the\n",
    "camera. \n",
    "\n",
    "Please see the application code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52d1a4",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Script to recognize, track, and estimate the \n",
    "# real world dimensions of an object within \n",
    "# 3m of distance from the camera\n",
    "# Author: Brenton Jackson\n",
    "# Date: 11/28/22\n",
    "\n",
    "\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "labelMap = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "            \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "nnPathDefault = str((Path(__file__).parent / Path('../depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')).resolve().absolute())\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('nnPath', nargs='?', help=\"Path to mobilenet detection network blob\", default=nnPathDefault)\n",
    "parser.add_argument('-ff', '--full_frame', action=\"store_true\", help=\"Perform tracking on full RGB frame\", default=False)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "fullFrameTracking = args.full_frame\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define sources and outputs\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)\n",
    "monoLeft = pipeline.create(dai.node.MonoCamera)\n",
    "monoRight = pipeline.create(dai.node.MonoCamera)\n",
    "stereo = pipeline.create(dai.node.StereoDepth)\n",
    "objectTracker = pipeline.create(dai.node.ObjectTracker)\n",
    "\n",
    "xoutRgb = pipeline.create(dai.node.XLinkOut)\n",
    "trackerOut = pipeline.create(dai.node.XLinkOut)\n",
    "\n",
    "xoutRgb.setStreamName(\"preview\")\n",
    "trackerOut.setStreamName(\"tracklets\")\n",
    "\n",
    "# Properties\n",
    "camRgb.setPreviewSize(300, 300)\n",
    "camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "camRgb.setInterleaved(False)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "\n",
    "# setting node configs\n",
    "stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)\n",
    "# Align depth map to the perspective of RGB camera, on which inference is done\n",
    "stereo.setDepthAlign(dai.CameraBoardSocket.RGB)\n",
    "stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())\n",
    "\n",
    "spatialDetectionNetwork.setBlobPath(args.nnPath)\n",
    "spatialDetectionNetwork.setConfidenceThreshold(0.5)\n",
    "spatialDetectionNetwork.input.setBlocking(False)\n",
    "spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)\n",
    "spatialDetectionNetwork.setDepthLowerThreshold(100)\n",
    "spatialDetectionNetwork.setDepthUpperThreshold(5000)\n",
    "\n",
    "objectTracker.setDetectionLabelsToTrack([5])  # track only person\n",
    "# possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF\n",
    "objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_COLOR_HISTOGRAM)\n",
    "# take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID\n",
    "objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)\n",
    "\n",
    "# Linking\n",
    "monoLeft.out.link(stereo.left)\n",
    "monoRight.out.link(stereo.right)\n",
    "\n",
    "camRgb.preview.link(spatialDetectionNetwork.input)\n",
    "objectTracker.passthroughTrackerFrame.link(xoutRgb.input)\n",
    "objectTracker.out.link(trackerOut.input)\n",
    "\n",
    "if fullFrameTracking:\n",
    "    camRgb.setPreviewKeepAspectRatio(False)\n",
    "    camRgb.video.link(objectTracker.inputTrackerFrame)\n",
    "    objectTracker.inputTrackerFrame.setBlocking(False)\n",
    "    # do not block the pipeline if it's too slow on full frame\n",
    "    objectTracker.inputTrackerFrame.setQueueSize(2)\n",
    "else:\n",
    "    spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)\n",
    "\n",
    "spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)\n",
    "spatialDetectionNetwork.out.link(objectTracker.inputDetections)\n",
    "stereo.depth.link(spatialDetectionNetwork.inputDepth)\n",
    "\n",
    "# Connect to device and start pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    preview = device.getOutputQueue(\"preview\", 4, False)\n",
    "    tracklets = device.getOutputQueue(\"tracklets\", 4, False)\n",
    "\n",
    "    startTime = time.monotonic()\n",
    "    counter = 0\n",
    "    fps = 0\n",
    "    color = (255, 255, 255)\n",
    "\n",
    "    while(True):\n",
    "        imgFrame = preview.get()\n",
    "        track = tracklets.get()\n",
    "\n",
    "        counter+=1\n",
    "        current_time = time.monotonic()\n",
    "        if (current_time - startTime) > 1 :\n",
    "            fps = counter / (current_time - startTime)\n",
    "            counter = 0\n",
    "            startTime = current_time\n",
    "\n",
    "        frame = imgFrame.getCvFrame()\n",
    "        trackletsData = track.tracklets\n",
    "        for t in trackletsData:\n",
    "            if int(t.spatialCoordinates.z) > 3000:\n",
    "                continue\n",
    "            roi = t.roi.denormalize(frame.shape[1], frame.shape[0])\n",
    "            x1 = int(roi.topLeft().x)\n",
    "            y1 = int(roi.topLeft().y)\n",
    "            x2 = int(roi.bottomRight().x)\n",
    "            y2 = int(roi.bottomRight().y)\n",
    "\n",
    "            width = abs(x2 - x1)\n",
    "            height = abs(y2 - y1)\n",
    "            focal_len = 457\n",
    "\n",
    "            try:\n",
    "                label = labelMap[t.label]\n",
    "            except:\n",
    "                label = t.label\n",
    "\n",
    "            cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"ID: {[t.id]}\", (x1 + 10, y1 + 35), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, t.status.name, (x1 + 10, y1 + 50), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)\n",
    "\n",
    "            cv2.putText(frame, f\"X: {int(t.spatialCoordinates.x)} mm\", (x1 + 10, y1 + 65), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Y: {int(t.spatialCoordinates.y)} mm\", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Z: {int(t.spatialCoordinates.z)} mm\", (x1 + 10, y1 + 95), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "\n",
    "            cv2.putText(frame, f\"Width: {ceil(width * int(t.spatialCoordinates.z) / focal_len) } mm\", (x2 - 100, y1 + 110), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Height: {ceil(height * int(t.spatialCoordinates.z) / focal_len)} mm\", (x2 - 100, y1 + 125), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "\n",
    "        cv2.putText(frame, \"NN fps: {:.2f}\".format(fps), (2, frame.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)\n",
    "\n",
    "        cv2.imshow(\"tracker\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2d1e2",
   "metadata": {},
   "source": [
    "*insert demo gif here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552c81a",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "Design an eco-friendly (try your best: as reusable as possible) “smart” business/visiting\n",
    "card (actual hardware) and an associated computer vision application using the camera\n",
    "provided. \n",
    "\n",
    "You can leverage depth information in your design.\n",
    "\n",
    "Please see the application code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc1acf2",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
