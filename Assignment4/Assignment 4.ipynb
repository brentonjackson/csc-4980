{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f51905",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a811783",
   "metadata": {},
   "source": [
    "Github repo for assignment: https://github.com/brentonjackson/csc-4980/tree/master/Assignment4\n",
    "\n",
    "I'll be using Python for the assignments in this class, as opposed to Matlab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3812e",
   "metadata": {},
   "source": [
    "## Part I\n",
    "\n",
    "Implement an application using the stereo camera where it will recognize, track and\n",
    "estimate dimensions of an object within 3m distance and inside field-of-view to the\n",
    "camera. \n",
    "\n",
    "Please see the application code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52d1a4",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Script to recognize, track, and estimate the \n",
    "# real world dimensions of an object within \n",
    "# 3m of distance from the camera\n",
    "# Author: Brenton Jackson\n",
    "# Date: 11/28/22\n",
    "\n",
    "\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "labelMap = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "            \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "nnPathDefault = str((Path(__file__).parent / Path('../depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')).resolve().absolute())\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('nnPath', nargs='?', help=\"Path to mobilenet detection network blob\", default=nnPathDefault)\n",
    "parser.add_argument('-ff', '--full_frame', action=\"store_true\", help=\"Perform tracking on full RGB frame\", default=False)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "fullFrameTracking = args.full_frame\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define sources and outputs\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)\n",
    "monoLeft = pipeline.create(dai.node.MonoCamera)\n",
    "monoRight = pipeline.create(dai.node.MonoCamera)\n",
    "stereo = pipeline.create(dai.node.StereoDepth)\n",
    "objectTracker = pipeline.create(dai.node.ObjectTracker)\n",
    "\n",
    "xoutRgb = pipeline.create(dai.node.XLinkOut)\n",
    "trackerOut = pipeline.create(dai.node.XLinkOut)\n",
    "\n",
    "xoutRgb.setStreamName(\"preview\")\n",
    "trackerOut.setStreamName(\"tracklets\")\n",
    "\n",
    "# Properties\n",
    "camRgb.setPreviewSize(300, 300)\n",
    "camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "camRgb.setInterleaved(False)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "\n",
    "# setting node configs\n",
    "stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)\n",
    "# Align depth map to the perspective of RGB camera, on which inference is done\n",
    "stereo.setDepthAlign(dai.CameraBoardSocket.RGB)\n",
    "stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())\n",
    "\n",
    "spatialDetectionNetwork.setBlobPath(args.nnPath)\n",
    "spatialDetectionNetwork.setConfidenceThreshold(0.5)\n",
    "spatialDetectionNetwork.input.setBlocking(False)\n",
    "spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)\n",
    "spatialDetectionNetwork.setDepthLowerThreshold(100)\n",
    "spatialDetectionNetwork.setDepthUpperThreshold(5000)\n",
    "\n",
    "objectTracker.setDetectionLabelsToTrack([5])  # track only person\n",
    "# possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF\n",
    "objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_COLOR_HISTOGRAM)\n",
    "# take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID\n",
    "objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)\n",
    "\n",
    "# Linking\n",
    "monoLeft.out.link(stereo.left)\n",
    "monoRight.out.link(stereo.right)\n",
    "\n",
    "camRgb.preview.link(spatialDetectionNetwork.input)\n",
    "objectTracker.passthroughTrackerFrame.link(xoutRgb.input)\n",
    "objectTracker.out.link(trackerOut.input)\n",
    "\n",
    "if fullFrameTracking:\n",
    "    camRgb.setPreviewKeepAspectRatio(False)\n",
    "    camRgb.video.link(objectTracker.inputTrackerFrame)\n",
    "    objectTracker.inputTrackerFrame.setBlocking(False)\n",
    "    # do not block the pipeline if it's too slow on full frame\n",
    "    objectTracker.inputTrackerFrame.setQueueSize(2)\n",
    "else:\n",
    "    spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)\n",
    "\n",
    "spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)\n",
    "spatialDetectionNetwork.out.link(objectTracker.inputDetections)\n",
    "stereo.depth.link(spatialDetectionNetwork.inputDepth)\n",
    "\n",
    "# Connect to device and start pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    preview = device.getOutputQueue(\"preview\", 4, False)\n",
    "    tracklets = device.getOutputQueue(\"tracklets\", 4, False)\n",
    "\n",
    "    startTime = time.monotonic()\n",
    "    counter = 0\n",
    "    fps = 0\n",
    "    color = (255, 255, 255)\n",
    "\n",
    "    while(True):\n",
    "        imgFrame = preview.get()\n",
    "        track = tracklets.get()\n",
    "\n",
    "        counter+=1\n",
    "        current_time = time.monotonic()\n",
    "        if (current_time - startTime) > 1 :\n",
    "            fps = counter / (current_time - startTime)\n",
    "            counter = 0\n",
    "            startTime = current_time\n",
    "\n",
    "        frame = imgFrame.getCvFrame()\n",
    "        trackletsData = track.tracklets\n",
    "        for t in trackletsData:\n",
    "            if int(t.spatialCoordinates.z) > 3000:\n",
    "                continue\n",
    "            roi = t.roi.denormalize(frame.shape[1], frame.shape[0])\n",
    "            x1 = int(roi.topLeft().x)\n",
    "            y1 = int(roi.topLeft().y)\n",
    "            x2 = int(roi.bottomRight().x)\n",
    "            y2 = int(roi.bottomRight().y)\n",
    "\n",
    "            width = abs(x2 - x1)\n",
    "            height = abs(y2 - y1)\n",
    "            focal_len = 457\n",
    "\n",
    "            try:\n",
    "                label = labelMap[t.label]\n",
    "            except:\n",
    "                label = t.label\n",
    "\n",
    "            cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"ID: {[t.id]}\", (x1 + 10, y1 + 35), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, t.status.name, (x1 + 10, y1 + 50), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)\n",
    "\n",
    "            cv2.putText(frame, f\"X: {int(t.spatialCoordinates.x)} mm\", (x1 + 10, y1 + 65), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Y: {int(t.spatialCoordinates.y)} mm\", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Z: {int(t.spatialCoordinates.z)} mm\", (x1 + 10, y1 + 95), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "\n",
    "            cv2.putText(frame, f\"Width: {ceil(width * int(t.spatialCoordinates.z) / focal_len) } mm\", (x2 - 100, y1 + 110), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Height: {ceil(height * int(t.spatialCoordinates.z) / focal_len)} mm\", (x2 - 100, y1 + 125), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "\n",
    "        cv2.putText(frame, \"NN fps: {:.2f}\".format(fps), (2, frame.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)\n",
    "\n",
    "        cv2.imshow(\"tracker\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96363dfb",
   "metadata": {},
   "source": [
    "This application tracks and measures the dimensions of bottles specifically.\n",
    "\n",
    "However, any of the objects in the labelMap array at the top of our scrip can be identified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab5baf",
   "metadata": {},
   "source": [
    "*insert demo gif here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552c81a",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "Design an eco-friendly (try your best: as reusable as possible) “smart” business/visiting\n",
    "card (actual hardware) and an associated computer vision application using the camera\n",
    "provided. \n",
    "\n",
    "You can leverage depth information in your design.\n",
    "\n",
    "Please see the application code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc1acf2",
   "metadata": {},
   "source": [
    "```python smart_card.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Script to recognize and perform actions\n",
    "# with smart card\n",
    "# Author: Brenton Jackson\n",
    "# Date: 11/28/22\n",
    "\n",
    "# The smart card system is a way to get more\n",
    "# insightful information about candidates in\n",
    "# a short amount of time, while also not putting\n",
    "# the burden of remembering every interaction\n",
    "# on recruiters.\n",
    "# It doesn't substitute face-to-face interaction,\n",
    "# but it adds an additional human element to a\n",
    "# candidate's resume.\n",
    "\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import time\n",
    "import argparse\n",
    "import subprocess\n",
    "\n",
    "labelMap = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "            \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "nnPathDefault = str((Path(__file__).parent / Path('../../depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')).resolve().absolute())\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('nnPath', nargs='?', help=\"Path to mobilenet detection network blob\", default=nnPathDefault)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define sources and outputs\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)\n",
    "monoLeft = pipeline.create(dai.node.MonoCamera)\n",
    "monoRight = pipeline.create(dai.node.MonoCamera)\n",
    "stereo = pipeline.create(dai.node.StereoDepth)\n",
    "objectTracker = pipeline.create(dai.node.ObjectTracker)\n",
    "\n",
    "xoutRgb = pipeline.create(dai.node.XLinkOut)\n",
    "trackerOut = pipeline.create(dai.node.XLinkOut)\n",
    "\n",
    "xoutRgb.setStreamName(\"preview\")\n",
    "trackerOut.setStreamName(\"tracklets\")\n",
    "\n",
    "# Properties\n",
    "camRgb.setPreviewSize(300, 300)\n",
    "camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "camRgb.setInterleaved(False)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "\n",
    "# setting node configs\n",
    "stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)\n",
    "# Align depth map to the perspective of RGB camera, on which inference is done\n",
    "stereo.setDepthAlign(dai.CameraBoardSocket.RGB)\n",
    "stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())\n",
    "\n",
    "spatialDetectionNetwork.setBlobPath(args.nnPath)\n",
    "spatialDetectionNetwork.setConfidenceThreshold(0.5)\n",
    "spatialDetectionNetwork.input.setBlocking(False)\n",
    "spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)\n",
    "spatialDetectionNetwork.setDepthLowerThreshold(100)\n",
    "spatialDetectionNetwork.setDepthUpperThreshold(5000)\n",
    "\n",
    "\n",
    "objectTracker.setDetectionLabelsToTrack([5, 15])  # track bottle or person\n",
    "# possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF\n",
    "objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_COLOR_HISTOGRAM)\n",
    "# take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID\n",
    "objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)\n",
    "\n",
    "# Linking\n",
    "monoLeft.out.link(stereo.left)\n",
    "monoRight.out.link(stereo.right)\n",
    "\n",
    "camRgb.preview.link(spatialDetectionNetwork.input)\n",
    "objectTracker.passthroughTrackerFrame.link(xoutRgb.input)\n",
    "objectTracker.out.link(trackerOut.input)\n",
    "\n",
    "\n",
    "spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)\n",
    "spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)\n",
    "spatialDetectionNetwork.out.link(objectTracker.inputDetections)\n",
    "stereo.depth.link(spatialDetectionNetwork.inputDepth)\n",
    "\n",
    "# Connect to device and start pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    preview = device.getOutputQueue(\"preview\", 4, False)\n",
    "    tracklets = device.getOutputQueue(\"tracklets\", 4, False)\n",
    "\n",
    "    startTime = time.monotonic()\n",
    "    counter = 0\n",
    "    fps = 0\n",
    "    color = (255, 255, 255)\n",
    "\n",
    "    frameNum = 0\n",
    "    while(True):\n",
    "        imgFrame = preview.get()\n",
    "        track = tracklets.get()\n",
    "\n",
    "        counter+=1\n",
    "        current_time = time.monotonic()\n",
    "        if (current_time - startTime) > 1 :\n",
    "            fps = counter / (current_time - startTime)\n",
    "            counter = 0\n",
    "            startTime = current_time\n",
    "\n",
    "        frame = imgFrame.getCvFrame()\n",
    "        # make copy of frame to preserve original value without\n",
    "        # object detection information overlayed\n",
    "        frame_orig = frame.copy() \n",
    "        trackletsData = track.tracklets\n",
    "        for t in trackletsData:            \n",
    "            roi = t.roi.denormalize(frame.shape[1], frame.shape[0])\n",
    "            x1 = int(roi.topLeft().x)\n",
    "            y1 = int(roi.topLeft().y)\n",
    "            x2 = int(roi.bottomRight().x)\n",
    "            y2 = int(roi.bottomRight().y)\n",
    "\n",
    "            width = abs(x2 - x1)\n",
    "            height = abs(y2 - y1)\n",
    "            focal_len = 457\n",
    "\n",
    "            # if card is close enough is detected\n",
    "            if int(t.spatialCoordinates.z) < 3000 and labelMap[t.label] == \"bottle\":\n",
    "                frameNum += 1\n",
    "                print(frameNum)\n",
    "                if frameNum > 30:\n",
    "                    # delay to let user stabilize the camera. upper limit on fps is 30,\n",
    "                    # so min delay would be 1 second after first detecting object\n",
    "                    # expected delay is ~2 seconds, since avg fps is ~15-25\n",
    "        \n",
    "                    filename = \"capture{}\".format(t.id)\n",
    "                    # create video, using -a flag to specify .mov file format\n",
    "                    # seems I must detach from this device to be able to connect again.\n",
    "                    device.close()\n",
    "                    cv2.imwrite(filename + \".png\", frame_orig)\n",
    "                    cv2.destroyWindow(\"tracker\") # close tracking window after capturing last frame\n",
    "                    print('recording starting in 5 seconds...')\n",
    "                    time.sleep(2)\n",
    "                    # has a couple additional second delay to start subprocess and run it\n",
    "                    ret = subprocess.run(['python3', 'record_video.py', '-n', filename, '-a'], check=True)\n",
    "                    if ret.returncode != 0:\n",
    "                        # error\n",
    "                        print(ret.returncode)\n",
    "                        print('video was not captured for {}'.format(filename))\n",
    "                    else:\n",
    "                        print('saved video successfully')\n",
    "                        print('quitting...')\n",
    "                        quit()\n",
    "            \n",
    "            try:\n",
    "                label = labelMap[t.label]\n",
    "            except:\n",
    "                label = t.label\n",
    "\n",
    "            cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"ID: {[t.id]}\", (x1 + 10, y1 + 35), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, t.status.name, (x1 + 10, y1 + 50), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)\n",
    "\n",
    "            cv2.putText(frame, f\"X: {int(t.spatialCoordinates.x)} mm\", (x1 + 10, y1 + 65), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Y: {int(t.spatialCoordinates.y)} mm\", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Z: {int(t.spatialCoordinates.z)} mm\", (x1 + 10, y1 + 95), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "\n",
    "            cv2.putText(frame, f\"Width: {ceil(width * int(t.spatialCoordinates.z) / focal_len) } mm\", (x2 - 100, y1 + 110), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "            cv2.putText(frame, f\"Height: {ceil(height * int(t.spatialCoordinates.z) / focal_len)} mm\", (x2 - 100, y1 + 125), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "\n",
    "        cv2.putText(frame, \"NN fps: {:.2f}\".format(fps), (2, frame.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)\n",
    "\n",
    "        cv2.imshow(\"tracker\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8825b2",
   "metadata": {},
   "source": [
    "Here is the helper module to record video:\n",
    "\n",
    "```python record_video.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Script to record video for\n",
    "# specified amount of time\n",
    "# Author: Brenton Jackson\n",
    "# Date: 12/1/22\n",
    "\n",
    "import argparse\n",
    "import cv2\n",
    "import time\n",
    "import depthai as dai\n",
    "from RGBPipeline import RGBPipeline\n",
    "import threading\n",
    "import subprocess\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write\n",
    "import os\n",
    "\n",
    "# parse command line arguments\n",
    "default_duration = 5\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('duration', nargs='?', help=\"Video length in seconds\", default=default_duration)\n",
    "parser.add_argument('-n', '--filename', nargs='?', help=\"Name of file to write to disk\", default='test')\n",
    "parser.add_argument('-a', '--apple', action=\"store_true\", help=\"If on Apple device (mac), change video codec\", default=False)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# global variables\n",
    "duration = int(args.duration)\n",
    "recording = None\n",
    "video_filename = \"\"\n",
    "startTime = 0\n",
    "elapsedTime = 0\n",
    "frameCount = 0\n",
    "expectedFps = 11.7 # may need to be fine-tuned for your computer, look at stdout for ***video fps***\n",
    "\n",
    "def record_vid():\n",
    "    global recording, video_filename, startTime, elapsedTime, frameCount, expectedFps\n",
    "    rgb_pipeline = RGBPipeline()\n",
    "    video_filename = args.filename + '.mp4'\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v') if args.apple else cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    out = cv2.VideoWriter(video_filename, fourcc, expectedFps, (1920, 1080))\n",
    "\n",
    "    # Connect to device and start pipeline\n",
    "    with dai.Device(rgb_pipeline.getPipeline()) as device:\n",
    "\n",
    "        video = device.getOutputQueue(rgb_pipeline.getStreamName(), maxSize=1, blocking=False)\n",
    "        startTime = time.monotonic()\n",
    "        print('recording video frames')\n",
    "        while recording == True or elapsedTime < duration:\n",
    "            videoIn = video.get()\n",
    "            vidFrame = videoIn.getCvFrame()\n",
    "            frameCount += 1\n",
    "            # Get BGR frame from NV12 encoded video frame to show with opencv\n",
    "            # Visualizing the frame on slower hosts might have overhead\n",
    "            \n",
    "            # write the frame\n",
    "            out.write(vidFrame)\n",
    "            # cv2.imshow('frame', vidFrame)\n",
    "            currTime = time.monotonic()\n",
    "            elapsedTime = currTime - startTime\n",
    "        print('video recording done')\n",
    "        recording = False\n",
    "        out.release()\n",
    "\n",
    "def record_audio():\n",
    "    global recording\n",
    "    rate = 44100\n",
    "    channels = 1\n",
    "    audio_filename = args.filename + '.wav'\n",
    "    # start stream, at the same time as video starts\n",
    "    recording = True\n",
    "    time.sleep(2) # delay to wait for video to start recording\n",
    "    print('recording audio')\n",
    "    myrecording = sd.rec(int(duration * rate), samplerate=rate, channels=channels)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    recording = False\n",
    "    print('audio recording done')\n",
    "    write(audio_filename, rate, myrecording)  # Save as WAV file \n",
    "    \n",
    "def start_AVrecording():\n",
    "    # play 3 beeps\n",
    "    data, fs = sf.read('beep.wav')\n",
    "    for i in range(0, 2):\n",
    "        sd.play(data, fs)\n",
    "        sd.wait()\n",
    "        time.sleep(0.9)\n",
    "    sd.play(data, fs)\n",
    "    # sd.wait()\n",
    "    \n",
    "    print('****  recording starting in 1s  ****')\n",
    "    threading.Thread(target=record_vid).start()\n",
    "    threading.Thread(target=record_audio).start()\n",
    "\n",
    "def file_manager(filename=args.filename):\n",
    "    # Processing of final files\n",
    "    print('processing files')\n",
    "    local_path = os.getcwd()\n",
    "    # if os.path.exists(str(local_path) + \"/\" + filename + \".wav\"):\n",
    "    #     os.remove(str(local_path) + \"/\" + filename + \".wav\")\n",
    "\n",
    "    if os.path.exists(str(local_path) + \"/\" + filename + \"2.mp4\"):\n",
    "        os.remove(str(local_path) + \"/\" + filename + \"2.mp4\")\n",
    "\n",
    "    if os.path.exists(str(local_path) + \"/\" + filename + \"_AV.mp4\"):\n",
    "        os.remove(str(local_path) + \"/\" + filename + \"_AV.mp4\")\n",
    "    print('file processing complete')\n",
    "\n",
    "def merge_files():\n",
    "    # merge two files after they're done\n",
    "    global video_filename, elapsedTime, frameCount, expectedFps\n",
    "    recordedFps = frameCount/elapsedTime\n",
    "    print('merging audio and video files')\n",
    "    print(\"*********   video fps: \" + str(recordedFps) + \"   ***********\")\n",
    "    print(\"fps difference: \" + str(abs(recordedFps - expectedFps)))\n",
    "    if abs(recordedFps - expectedFps) >= 0.01:  # If the fps rate was higher/lower than expected, re-encode it to the expected\n",
    "        print(\"Re-encoding\")\n",
    "        tempFilename = args.filename + \"2.mp4\"\n",
    "        cmd = \"ffmpeg -r \" + str(recordedFps) + \" -i \" + video_filename + \" -r \" + str(expectedFps) + \" -b:v 6000k -vcodec mpeg4 \" + tempFilename\n",
    "        subprocess.call(cmd, shell=True)\n",
    "\n",
    "        print(\"Muxing\")\n",
    "        cmd = \"ffmpeg -ac 1 -channel_layout stereo -i \" + args.filename + \".wav -i \" + tempFilename + \" -b:v 6000k -vcodec mpeg4 \" + args.filename + \"_AV.mp4\"\n",
    "        retcode = subprocess.call(cmd, shell=True)\n",
    "\n",
    "    else:\n",
    "        print(\"Muxing\")\n",
    "        cmd = \"ffmpeg -ac 1 -channel_layout stereo -i \" + args.filename + \".wav -i \" + video_filename + \" -b:v 6000k -vcodec mpeg4 \" + args.filename + \"_AV.mp4\"\n",
    "        retcode = subprocess.call(cmd, shell=True)\n",
    "    \n",
    "    if not retcode:\n",
    "        print(\"done merging the files\")\n",
    "    else:\n",
    "        print(\"video and audio merge failed\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # main control flow\n",
    "    start_AVrecording()\n",
    "    time.sleep(duration + 5) # 5s buffer to make sure everything is done\n",
    "    file_manager()\n",
    "    time.sleep(5) # time for os to handle files\n",
    "    merge_files()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0450fe",
   "metadata": {},
   "source": [
    "And here is the helper to the helper, RGBPipeline.py, which just saves some code by putting the DepthAI pipeline creation into its own class.\n",
    "\n",
    "```python\n",
    "import depthai as dai\n",
    "\n",
    "class RGBPipeline:\n",
    "    'Base class for DepthAI RGB camera pipeline'\n",
    "\n",
    "    def __init__(self, streamName=\"video\") -> None:\n",
    "        self.streamName = streamName\n",
    "        # Create pipeline\n",
    "        self.pipeline = dai.Pipeline()\n",
    "\n",
    "        # Define source and output\n",
    "        camRgb = self.pipeline.create(dai.node.ColorCamera)\n",
    "        xoutVideo = self.pipeline.create(dai.node.XLinkOut)\n",
    "        xoutVideo.setStreamName(streamName)\n",
    "        \n",
    "        # Properties\n",
    "        camRgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "        camRgb.setVideoSize(1920, 1080)\n",
    "\n",
    "        xoutVideo.input.setBlocking(False)\n",
    "        xoutVideo.input.setQueueSize(1)\n",
    "\n",
    "        # Linking\n",
    "        camRgb.video.link(xoutVideo.input)\n",
    "\n",
    "    def getPipeline(self):\n",
    "        return self.pipeline\n",
    "\n",
    "    def getStreamName(self):\n",
    "        return self.streamName\n",
    "   \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc7606",
   "metadata": {},
   "source": [
    "## Approach / Ideas\n",
    "\n",
    "We thought of this prompt from a number of approaches.\n",
    "\n",
    "One thought that stood out in this situation was the scenario of being at a career fair, both as a job seeker and an employer. That yielded a few ideas:\n",
    "\n",
    "On the employer end:\n",
    "- Create an application that used depth to trigger taking a snapshot of the business card, then subsequently allow the job seeker to record a 20-30s elevator pitch. All of this would be done automatically and saved either on the filesystem or in a database to be used in a web app. The web app's main purpose would be to serve as an easy interface for organization.\n",
    "\n",
    "That idea was a combination of a few ideas:\n",
    "- Using depth to save pic of card to database when it got close enough to the camera\n",
    "- Using depth to trigger a prompt to record the voice of the candidate\n",
    "- Using depth to trigger a prompt to record video of the candidate\n",
    "\n",
    "The intended setting for this system would be as a stationary stand by or behind your booth, as an optional task.\n",
    "In this way, we know we're getting enough power to power everything and having the camera be stationary instead of having this application use a mobile camera adds reliability to the system.\n",
    "\n",
    "We had other ideas in the scenario that our computer vision software was running on a cell phone, but decided to focus on the former scenario instead of the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8642d33",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "To merge the audio and video signals, you need to install [ffmpeg](https://ffmpeg.org/) first.\n",
    "\n",
    "You're also going to want to run video_recorder.py once to get an accurate idea of the actual fps your camera is running at. \n",
    "\n",
    "Then fine-tune the value on line 34 based on the log output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90a971",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "Unfortunately, this prototype is not complete.\n",
    "\n",
    "The most obvious point of improvement would be in the object recognition implementation.\n",
    "\n",
    "We need to detect actual business cards or resumes.\n",
    "\n",
    "In addition, it would be convenient to perform OCR on the card and use the candidate's name as the filename. My partner implemented the optical character recognition feature in his prototype.\n",
    "\n",
    "I did explore using traditional computer vision methods, such as getting the contours of the frame and drawing the edges to detect business cards, but that method didn't work in real-time where I was performing that operation on every frame. It just wasn't accurate enough.\n",
    "\n",
    "I could have managed the time a little better to be able to make a new model that recognized business cards.\n",
    "Instead, I skipped that step and assumed it was trivial for the purpose of finishing and proving the proof of concept - which I believe we did.\n",
    "\n",
    "Another improvement to the application would be to make it run continuously. In this implementation, everything\n",
    "closes after the video is recorded. This is due in part to the fact that I have to disconnect from the device in order to open it again for the video recording.\n",
    "\n",
    "Now, I could implement a goto action or wrap the entire program in a loop to run continuously. I'm sure there are smarter and more sophisticated ways to go about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf7220",
   "metadata": {},
   "source": [
    "*insert demo gif here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
