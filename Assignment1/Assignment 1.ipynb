{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4aa6b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f358249",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Github repo for assignment: https://github.com/brentonjackson/csc-4980/tree/master/Assignment1\n",
    "\n",
    "I'll be using Python for the assignments in this class, as opposed to Matlab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa301a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part A: Fundamentals\n",
    "\n",
    "Go over camera calibration toolbox and calibrate camera.\n",
    "\n",
    "It may be worth mentioning that in the [DepthAI documentation](https://docs.luxonis.com/projects/hardware/en/latest/pages/guides/calibration.html), for the nonmodular cameras, they've already been calibrated before shipment so recalibration isn't needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024530b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, I've calibrated the camera by following directions at this link as a learning exercise: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\n",
    "\n",
    "Below is the code I used to do this, in Python (skipped in slideshow), and before and after images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4aef310",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Camera calibration for the OAK-D Lite camera (or any camera)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import glob\n",
    "\n",
    "# 1. get object points and image points\n",
    "# termination criteria\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*7, 3), np.float32)\n",
    "objp[:, :2] = np.mgrid[0:7, 0:6].T.reshape(-1, 2)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = []  # 3d point in real world space\n",
    "imgpoints = []  # 2d points in image plane.\n",
    "images = glob.glob('../../opencv-samples/left*.jpg')\n",
    "for fname in images:\n",
    "    img = cv.imread(fname)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, (7, 6), None)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "        # Draw and display the corners\n",
    "        cv.drawChessboardCorners(img, (7, 6), corners2, ret)\n",
    "        cv.imshow('img', img)\n",
    "        cv.waitKey(500)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "# 2. calibrate camera\n",
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "# 3. undistort image\n",
    "img = cv.imread('../../opencv-samples/left12.jpg')\n",
    "h,  w = img.shape[:2]\n",
    "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))\n",
    "\n",
    "# undistort\n",
    "dst = cv.undistort(img, mtx, dist, None, newcameramtx)\n",
    "# crop the image\n",
    "x, y, w, h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "cv.imwrite('calibresult.png', dst)\n",
    "cv.imshow('calibrate result', dst)\n",
    "cv.waitKey(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01deeb62",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Distorted image:\n",
    "\n",
    "![Distorted image](https://github.com/brentonjackson/csc-4980/blob/master/opencv-samples/left12.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fd261",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Image after calibration and undistortion:\n",
    "\n",
    "![Undistorted image](Part-1/calibresult.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285bd40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see from the curved lines in the first image, there was a considerable amount of radial distortion present.\n",
    "\n",
    "That was fixed in the latter image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f8660",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part B: Matlab/Python Prototyping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd864d44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Write a MATLAB/Python script to find the real world dimensions (e.g. diameter of a ball, side length of a cube) of an object using perspective projection equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a478e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Validate using an experiment where you image an object using your camera from a specific distance (choose any distance but ensure you are able to measure it accurately) between the object and camera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b645d",
   "metadata": {},
   "source": [
    "This assignment requires some background to understand before implementing in code.\n",
    "\n",
    "## Perspective Projection\n",
    "\n",
    "### Background\n",
    "\n",
    "In this example, I use the pinhole camera model to understand perspective projection.\n",
    "\n",
    "In the previous example, we learned that camera calibration required a few things:\n",
    "- **Extrinsic parameters** of the camera, e.g. rotation and translation vectors, which translates a coordinate of a 3D point to a coordinate system\n",
    "- **Intrinsic parameters** of the camera, e.g. focal length and optical centers (both given in the form of a camera matrix) - visit [this link](https://ksimek.github.io/2013/08/13/intrinsic/) for a great breakdown on the intrinsic params\n",
    "- **Distortion coefficients**\n",
    "\n",
    "We used images of a chess board to find the camera matrix values since we knew the relative positions of the square corners on the board. By doing this, we were able to find intrinsic parameters of the OAK-D Lite camera.\n",
    "\n",
    "We can use those parameters (e.g. focal point and optical centers) to undistort any image taken with the camera.\n",
    "\n",
    "As will be explained in a second, we can also use one of the parameters to help us find real world coordinates of our object from 2d image coordinates. That's the goal of using perspective projection equations.\n",
    "\n",
    "\n",
    "### Use of Perspective Projection Equations\n",
    "\n",
    "The general play-by-play of part B will be to:\n",
    "1. Undistort our image\n",
    "2. Find some desired dimension in our 2D image, like height or width\n",
    "3. Use perspective projection equations to convert our desired dimension to 3D, real world values and units\n",
    "\n",
    "We know how to do 1. We've already done it, so we can modify our camera calibration script from Part A to accept any image, undistort it, then write that new image to the disk for us.\n",
    "\n",
    "For 2, we can go about it in two ways:\n",
    "1. Calculate the 2D dimensions of the object(s) in our image using object outlines, bounding boxes, and calculated Euclidean distances of those bounding box sides\n",
    "2. Allow the user to specify two points of interest and use those points to calculate the 2D dimensions of interest\n",
    "\n",
    "I will opt for method 2, since it's very easy to test in the real-world.\n",
    "\n",
    "For 3, this is where we actually make use of the equations which I will go over in the next section.\n",
    "\n",
    "### Perspective Projection Equations\n",
    "\n",
    "The perspective projection equations are equations that allow us to convert coordinates on the image plane to coordinates in the real world, and vice-versa. It uses the concept of similar triangles to essentially create a ratio. It's better explained with an image:\n",
    "\n",
    "![Perspective Projection image](perspective_projection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c30b6",
   "metadata": {},
   "source": [
    "We can treat the middle plane as our camera and the image plane on the left as the computer screen our image is rendered on.\n",
    "\n",
    "We want to convert the point P_i on the image plane to the point P_0 in the real world on the right. To do this, we see that the optical axis sets up similar triangles. \n",
    "\n",
    "From these equations, we can see that **a point in the real world depends on the ratio of the object distance from the camera to the focal length, times the corresponding coordinate on the image plane**. Since we know the 2D image coordinates and the focal length of the camera, which we got from our camera matrix, we only need to know the distance of the object from the camera.\n",
    "\n",
    "We can give that distance to our program as a required parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258a7e1",
   "metadata": {},
   "source": [
    "Below is the code that achieves this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e3d43",
   "metadata": {},
   "source": [
    "find_obj_dims.py\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Script to find real world dimensions of an object from image\n",
    "# Author: Brenton Jackson\n",
    "# Date: 11/25/22\n",
    "\n",
    "# parameters needed (inputs):\n",
    "# image\n",
    "# viewport distance (real-world) from camera dist\n",
    "\n",
    "# desired output:\n",
    "# viewport dist corresponding to dist from 2d image\n",
    "\n",
    "\n",
    "import argparse\n",
    "import subprocess\n",
    "from dist_between_pts import get_dims_2D\n",
    "from calibrate_camera import newcameramtx as camera_matrix # import calculated intrinsic and extrinsic parameters of camera\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required=False, help=\"Path to the image\")\n",
    "ap.add_argument(\"-d\", \"--distance\", required=False, help=\"Distance of object from camera\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# 1. Grab image with object in it\n",
    "# if no image argument specified, capture new image\n",
    "if (not args[\"image\"]) :\n",
    "    subprocess.run(['python3', 'capture_img.py'])\n",
    "imgName = args[\"image\"] or \"opencv_frame1.png\"\n",
    "dist = args[\"distance\"] or 15 # distance from camera in inches\n",
    "dist = int(dist)\n",
    "\n",
    "# 2. Find height of object (in image) by allowing user to select two points on image\n",
    "dist_image = get_dims_2D(imgName=imgName)\n",
    "\n",
    "\n",
    "# 3. Use perspective projection equations to convert our desired dimension to 3D, real world values and units\n",
    "# equation: real_dist = dist_image * dist_from_camera / focal_len\n",
    "focal_len = (camera_matrix[0][0] + camera_matrix[1][1]) / 2 # avg of fx and fy\n",
    "real_dist = dist_image * dist / focal_len\n",
    "\n",
    "print(\"real world distance: \", real_dist, '(in dist units)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35d5d3",
   "metadata": {},
   "source": [
    "\n",
    "This is the main script that follows our algorithm one-to-one.\n",
    "\n",
    "I use some helper functions and abstract out some code to make it cleaner. I'll post the imported code to implement step 2 below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accbf77",
   "metadata": {},
   "source": [
    "dist_between_pts.py\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Click two points on an image and calculate Euclidean distance\n",
    "# Author: Brenton Jackson\n",
    "# Date: 11/27/22\n",
    "\n",
    "from math import sqrt\n",
    "import cv2\n",
    "\n",
    "\n",
    "#define global variables for mouse callback function capture_pts\n",
    "clickOne = False\n",
    "clickTwo = False\n",
    "imgPts = []\n",
    "clone = []\n",
    "image = []\n",
    "circleRadius = 3\n",
    "circleColor = (0, 255, 0) # green\n",
    "lineThickness = 4\n",
    "lineColor = (0, 0, 255) # red\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def capture_pts(event, x, y, flags, param):\n",
    "    \"\"\" \n",
    "    Callback function that allows user to click two points in image to calculate distance\n",
    "\n",
    "    @param event The event that took place (left mouse button pressed, left mouse button released, mouse movement, etc)\n",
    "    @param x The x-coordinate of the event\n",
    "    @param y The y-coordinate of the event\n",
    "    @param flags Any relevant flags passed by OpenCV\n",
    "    @param param Any extra parameters supplied by OpenCV\n",
    "\n",
    "    return No return value. Modifies global imgPts array\n",
    "    \"\"\"\n",
    "\n",
    "    # grab references to the global variables\n",
    "    global clickOne, clickTwo, imgPts, clone, image\n",
    " \n",
    "    \n",
    "\n",
    "\t# if the left mouse button was clicked, record the \n",
    "\t# (x, y) coordinates and indicate that first click is\n",
    "\t# being captured\n",
    "    if event == cv2.EVENT_LBUTTONUP and clickOne == True:\n",
    "        imgPts = [(x, y)]\n",
    "        # draw circle around the region of interest\n",
    "        cv2.circle(clone, (x, y), circleRadius, circleColor, -1)\n",
    "        cv2.imshow(\"image\", clone)\n",
    "\n",
    "        k = cv2.waitKey(0)\n",
    "        \n",
    "        # reset points if r is pressed\n",
    "        if k == ord(\"r\"):\n",
    "            refresh_image()\n",
    "\n",
    "        if k%256 == 32:\n",
    "            # SPACE pressed, break out\n",
    "            clickOne = False\n",
    "            clickTwo = True\n",
    "            print(\"click second coordinate\")\n",
    "    \n",
    "    # if the left mouse button was clicked, record the \n",
    "\t# (x, y) coordinates of the second point and indicate \n",
    "    # that the operation is complete\n",
    "    if event == cv2.EVENT_LBUTTONUP and clickTwo == True:\n",
    "        # draw circle around the region of interest\n",
    "        cv2.circle(clone, (x, y), circleRadius, circleColor, -1)\n",
    "        cv2.imshow(\"image\", clone)\n",
    "\n",
    "        k = cv2.waitKey(0)\n",
    "        \n",
    "        # reset points if r is pressed\n",
    "        if k == ord(\"r\"):\n",
    "            refresh_image()\n",
    "\n",
    "        if k%256 == 32:\n",
    "            # SPACE pressed, break out\n",
    "            clickTwo = False\n",
    "            imgPts.append((x, y))\n",
    "            print(imgPts)\n",
    "            print(\"captured both coordinates\")\n",
    "\n",
    "def refresh_image():\n",
    "    \"\"\" \n",
    "    Function that resets the image so it's clear of points\n",
    "    and resets the array of image points\n",
    "\n",
    "    return No return value. Modifies global imgPts array,\n",
    "    clickOne flag, clickTwo flag, and clone image variable\n",
    "    \"\"\"\n",
    "    global imgPts, clickOne, clickTwo, clone, image\n",
    "    clone = image.copy()\n",
    "    imgPts = []\n",
    "    clickOne = True\n",
    "    clickTwo = False\n",
    "\n",
    "def calculate_dist(points):\n",
    "    \"\"\"\n",
    "    Function that takes two (x,y) coordinates and calculates\n",
    "    the euclidean distance between them\n",
    "\n",
    "    @param points An array containing two tuples\n",
    "    \"\"\"\n",
    "    (x1, y1) = points[0]\n",
    "    (x2, y2) = points[1]\n",
    "    return round(sqrt((x1-x2)**2 + (y1-y2)**2))\n",
    "\n",
    "def get_dims_2D(imgName):\n",
    "    \"\"\"\n",
    "    Function to get allow user to get distance between\n",
    "    two points on object in an image\n",
    "\n",
    "    @return Distance between points in pixels\n",
    "\n",
    "    @param imgName Name of desired image with object\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    Click a point on image, then press SPACE to confirm point.\n",
    "    \n",
    "    To refresh the image, press 'R'\n",
    "\n",
    "    After confirming two points, press 'C' to continue and a\n",
    "    line connecting the points will be drawn in addition to\n",
    "    the distance being calculated.\n",
    "\n",
    "    Press 'Q' to close image windows when done.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    global imgPts, clickOne, clickTwo, clone, image\n",
    "\n",
    "    # load the image, clone it, and setup the mouse callback function\n",
    "    image = cv2.imread(imgName)\n",
    "    clone = image.copy()\n",
    "    cv2.namedWindow(\"image\")\n",
    "    cv2.setMouseCallback(\"image\", capture_pts)\n",
    "    \n",
    "    # loop until we've confirmed our two points\n",
    "    clickOne = True\n",
    "    while True:\n",
    "        # display the image and wait for a keypress\n",
    "        cv2.imshow(\"image\", clone)\n",
    "        key = cv2.waitKey(1)\n",
    "        \n",
    "        # if the 'r' key is pressed, reset the points\n",
    "        if key == ord(\"r\"):\n",
    "            refresh_image()\n",
    "        # if the 'c' key is pressed, break from the loop\n",
    "        elif key == ord(\"c\"):\n",
    "            break\n",
    "        elif len(imgPts) >= 2:\n",
    "            break    \n",
    "    \n",
    "    # if there are two reference points, then draw the line\n",
    "    # between the points on the image clone\n",
    "    dist = 0\n",
    "    if len(imgPts) == 2:\n",
    "        while True:\n",
    "            cv2.line(clone, imgPts[0], imgPts[1], lineColor, lineThickness)\n",
    "            cv2.imshow(\"Distance\", clone)\n",
    "            k = cv2.waitKey(1)\n",
    "            if k == ord('q'):\n",
    "                break\n",
    "        dist = calculate_dist([imgPts[0], imgPts[1]])\n",
    "    \n",
    "\n",
    "    print(\"dist: \", dist, \"px\")\n",
    "    return dist\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286174c",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "There are a few things to note about my implementation.\n",
    "\n",
    "To run the script, you're expected to add an image name (undistorted ideally) and a distance as arguments.\n",
    "\n",
    "When you run it, an image window will popup. \n",
    "You're expected to click on two points, pressing SPACE bar to confirm the points.\n",
    "After confirming, you will see the distance line segment between the points.\n",
    "\n",
    "\n",
    "For my focal length, I used the average of the two focal length values given in our camera matrix.\n",
    "\n",
    "**Actual real-world dimensions using this script has a margin of error due to:**\n",
    "- Correctness of the distance provided\n",
    "- How distorted or accurate the image is\n",
    "- How accurately you placed your two points in the image\n",
    "- The angle of the camera in relation to the object in the image\n",
    "\n",
    "\n",
    "With that being said, I used the script on an image I took yesterday, in which I estimated the distance based on my usual sitting position and the position of my mug in the picture.\n",
    "I measured my mug's height at ~5.3125\". When I ran the script, it gave me a height of 5.38\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0614443",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "![Link](./part2_demo.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f578a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part C: Application Development\n",
    "\n",
    "Setup your application to show a RGB stream from the mono camera and a depth map stream from the stereo camera simultaneously. \n",
    "\n",
    "Is it feasible? \n",
    "\n",
    "What is the maximum frame rate and resolution achievable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ccceb",
   "metadata": {},
   "source": [
    "\n",
    "Below is the code of the application I created to accomplish this task.\n",
    "\n",
    "**It is feasible**, and not only is it feasible, so much more can be done in addition to showing the two streams simultaneously.\n",
    "\n",
    "With depthai, you can show however many streams you want, as long as you're okay with the limitations that are inherent in that.\n",
    "\n",
    "Since the entire API is centered around the concept of the device running a pipeline, you can add whatever nodes to the pipeline you want. With depth-ai, any camera is a node. There is a Node for the RGB stream, as well as Nodes for the depth map stream. Connect both those Nodes to output nodes and that allows users to view the data from them.\n",
    "\n",
    "The trade-off there, however, is additional lag. That's to be expected with more computations. \n",
    "\n",
    "DepthAI has posted some performance benchmarks to go by as a [reference](https://docs.luxonis.com/projects/api/en/latest/tutorials/low-latency/).\n",
    "\n",
    "I wasn't able to measure the framerates from the camera exactly, so the results may vary. However, in my application, I found that the **maximum resolution allowed for the mono cameras was 480P, and the maximum FPS was 30FPS**. It responded pretty smoothly.\n",
    "\n",
    "For the Color camera, the **max resolution was 4K**, although that limited the framerate to 28.8FPS. **At 1080P, 30FPS was supported**. I noticed more latency here, but it was still acceptable.\n",
    "\n",
    "Again, I didn't calculate the FPS myself. This was just the information that the API exposed to me via the getFPS() method on each of the cameras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9697a",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Script to show RGB stream and depth map stream\n",
    "# from camera simultaneously\n",
    "# Author: Brenton Jackson\n",
    "# Date: 11/28/22\n",
    "\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "res = dai.MonoCameraProperties.SensorResolution.THE_480_P\n",
    "median = dai.StereoDepthProperties.MedianFilter.KERNEL_7x7\n",
    "\n",
    "def getDisparityFrame(frame):\n",
    "    maxDisp = stereo.initialConfig.getMaxDisparity()\n",
    "    disp = (frame * (255.0 / maxDisp)).astype(np.uint8)\n",
    "    disp = cv2.applyColorMap(disp, cv2.COLORMAP_JET)\n",
    "\n",
    "    return disp\n",
    "\n",
    "# Create pipeline with RGB and stereo depth\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define source and output for RGB\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "xoutVideo = pipeline.create(dai.node.XLinkOut)\n",
    "xoutVideo.setStreamName(\"video\")\n",
    "\n",
    "# Define source and outputs for stereo depth\n",
    "camLeft = pipeline.create(dai.node.MonoCamera)\n",
    "camRight = pipeline.create(dai.node.MonoCamera)\n",
    "stereo = pipeline.create(dai.node.StereoDepth)\n",
    "xoutLeft = pipeline.create(dai.node.XLinkOut)\n",
    "xoutRight = pipeline.create(dai.node.XLinkOut)\n",
    "xoutDisparity = pipeline.create(dai.node.XLinkOut)\n",
    "\n",
    "\n",
    "# RGB Properties\n",
    "camRgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "camRgb.setVideoSize(1920, 1080)\n",
    "\n",
    "# Stereo Depth Properties\n",
    "camLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "camRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "for monoCam in (camLeft, camRight):\n",
    "    monoCam.setResolution(res)\n",
    "\n",
    "stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)\n",
    "stereo.initialConfig.setMedianFilter(median)  # KERNEL_7x7 default\n",
    "stereo.setRectifyEdgeFillColor(0)  # Black, to better see the cutout\n",
    "\n",
    "xoutLeft.setStreamName(\"left\")\n",
    "xoutRight.setStreamName(\"right\")\n",
    "xoutDisparity.setStreamName(\"disparity\")\n",
    "\n",
    "\n",
    "# Linking\n",
    "camRgb.video.link(xoutVideo.input)\n",
    "\n",
    "\n",
    "camLeft.out.link(stereo.left)\n",
    "camRight.out.link(stereo.right)\n",
    "stereo.syncedLeft.link(xoutLeft.input)\n",
    "stereo.syncedRight.link(xoutRight.input)\n",
    "stereo.disparity.link(xoutDisparity.input)\n",
    "\n",
    "# set stream names for depth streams\n",
    "streams = [\"left\", \"right\", \"disparity\"]\n",
    "\n",
    "\n",
    "\n",
    "# Connect to device and start pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "    print(\"RGB Framerate: \", camRgb.getFps())\n",
    "    print(\"Depth Framerate: \", camLeft.getFps())\n",
    "    # queue for RGB\n",
    "    video = device.getOutputQueue(name=\"video\", maxSize=1, blocking=False)\n",
    "    \n",
    "    # queues for stereo depth streams\n",
    "    qList = [device.getOutputQueue(stream, 8, blocking=False) for stream in streams]\n",
    "    \n",
    "    while True:\n",
    "        # show RGB video\n",
    "        videoIn = video.get()\n",
    "        # Get BGR frame from NV12 encoded video frame to show with opencv\n",
    "        # Visualizing the frame on slower hosts might have overhead\n",
    "        cv2.imshow(\"video\", videoIn.getCvFrame())\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # show depth stream video along with left and right mono cameras\n",
    "        for q in qList:\n",
    "            name = q.getName()\n",
    "            frame = q.get().getCvFrame()\n",
    "            if name == \"depth\":\n",
    "                frame = frame.astype(np.uint16)\n",
    "            elif name == \"disparity\":\n",
    "                frame = getDisparityFrame(frame)\n",
    "\n",
    "            cv2.imshow(name, frame)\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
