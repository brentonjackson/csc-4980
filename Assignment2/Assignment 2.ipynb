{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb487000",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f358249",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Github repo for assignment: https://github.com/brentonjackson/csc-4980/tree/master/Assignment2\n",
    "\n",
    "I'll be using Python for the assignments in this class, as opposed to Matlab.\n",
    "\n",
    "**Credit goes to Amani Hunter for assets and code**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa301a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part I:\n",
    "\n",
    "1. Capture 10s handheld video footage and pan from left to right or right to left.\n",
    "\n",
    "\n",
    "2. Convert all frames to grayscale. \n",
    "\n",
    "    - Pick one frame and find the boundary of any object using Harris corner and Canny edge detection.\n",
    "   \n",
    "\n",
    "3. Pick another image frame that has the same object in view as the previous image.\n",
    "    \n",
    "   - Find all corresponding points of the object between the two images.\n",
    "   \n",
    "   - Find the homography matrix between the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca65ad",
   "metadata": {},
   "source": [
    "Below is the code and resulting images from the Harris corner and Canny edge detection functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54f3fa",
   "metadata": {},
   "source": [
    "**Recorder.py** records the video footage and allows the user to choose specific frames by pressing SPACE bar\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import depthai as dai\n",
    "\n",
    "streams = []\n",
    "streams.append('isp')\n",
    "\n",
    "@nb.njit(nb.uint16[::1] (nb.uint8[::1], nb.uint16[::1], nb.boolean), parallel=True, cache=True)\n",
    "def unpack_raw10(input, out, expand16bit):\n",
    "    lShift = 6 if expand16bit else 0\n",
    "\n",
    "    for i in np.arange(input.size // 5):\n",
    "        b4 = input[i * 5 + 4]\n",
    "        out[i * 4]     = ((input[i * 5]     << 2) | ( b4       & 0x3)) << lShift\n",
    "        out[i * 4 + 1] = ((input[i * 5 + 1] << 2) | ((b4 >> 2) & 0x3)) << lShift\n",
    "        out[i * 4 + 2] = ((input[i * 5 + 2] << 2) | ((b4 >> 4) & 0x3)) << lShift\n",
    "        out[i * 4 + 3] = ((input[i * 5 + 3] << 2) |  (b4 >> 6)       ) << lShift\n",
    "\n",
    "    return out\n",
    "\n",
    "print(\"depthai version:\", dai.__version__)\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "cam = pipeline.createColorCamera()\n",
    "cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_12_MP)\n",
    "\n",
    "if 'isp' in streams:\n",
    "    xout_isp = pipeline.createXLinkOut()\n",
    "    xout_isp.setStreamName('isp')\n",
    "    cam.isp.link(xout_isp.input)\n",
    "\n",
    "device = dai.Device(pipeline)\n",
    "device.startPipeline()\n",
    "\n",
    "q_list = []\n",
    "for s in streams:\n",
    "    q = device.getOutputQueue(name=s, maxSize=3, blocking=True)\n",
    "    q_list.append(q)\n",
    "    cv2.namedWindow(s, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(s, (960, 540))\n",
    "\n",
    "capture_flag = False\n",
    "image_counter = 0\n",
    "while True:\n",
    "    for q in q_list:\n",
    "        name = q.getName()\n",
    "        data = q.get()\n",
    "        width, height = data.getWidth(), data.getHeight()\n",
    "        payload = data.getData()\n",
    "        capture_file_info_name = f\"Amani_capture_{name}_{image_counter}\"\n",
    "        if name == 'isp':\n",
    "            shape = (height * 3 // 2, width)\n",
    "            yuv420p = payload.reshape(shape).astype(np.uint8)\n",
    "            bgr = cv2.cvtColor(yuv420p, cv2.COLOR_YUV2BGR_IYUV)\n",
    "            grayscale_img = cv2.cvtColor(bgr,cv2.COLOR_BGR2GRAY)\n",
    "        if capture_flag:\n",
    "            filename = capture_file_info_name + '.png'\n",
    "            grayscale_img = np.ascontiguousarray(grayscale_img)\n",
    "            cv2.imwrite(filename, grayscale_img)\n",
    "        bgr = np.ascontiguousarray(bgr)\n",
    "        cv2.imshow(name, grayscale_img)\n",
    "    capture_flag = False\n",
    "    key = cv2.waitKey(5)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key%256 == 32:\n",
    "        capture_flag = True\n",
    "        image_counter += 1\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71605d66",
   "metadata": {},
   "source": [
    "**HarrisCorner.py**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "captured_image = cv2.imread(\"Amani_capture_isp_2.png\")\n",
    "grayImg = cv2.cvtColor(captured_image, cv2.COLOR_BGR2GRAY)\n",
    "grayImg = np.float32(grayImg)\n",
    "dest = cv2.cornerHarris(grayImg, 8, 29, 0.05)\n",
    "dest = cv2.dilate(dest, None)\n",
    "captured_image[dest > 0.01 * dest.max()] = [0, 0, 255]\n",
    "cv2.imshow('HarrisCorners', captured_image)\n",
    "cv2.imwrite(\"HarrisCorners.png\", captured_image)\n",
    "\n",
    "plt.imshow(captured_image)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**CannyEdgeDetection.py**:\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "captured_image = cv2.imread(\"Amani_capture_isp_1.png\",0)\n",
    "edges = cv2.Canny(captured_image, 180, 200)\n",
    "plt.subplot(121),plt.imshow(captured_image,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b8a1b",
   "metadata": {},
   "source": [
    "Input image:\n",
    "\n",
    "![Input image](Part-1/Amani_capture_isp_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b470767",
   "metadata": {},
   "source": [
    "Output image:\n",
    "\n",
    "![Output image](Part-1/HarrisCorners.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b6c63",
   "metadata": {},
   "source": [
    "**Coordinates.py** allows the user to click on the 4 coordinates of the object in both of the images.\n",
    "The coordinates will be used to find the homography matrix.\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "def clickImage(event, x, y, flags, params):\n",
    "\tif event == cv2.EVENT_LBUTTONDOWN:\n",
    "\t\tprint(x, ' ', y)\n",
    "\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\t\tcv2.putText(img, str(x) + ',' +\n",
    "\t\t\t\t\tstr(y), (x,y), font,\n",
    "\t\t\t\t\t1, (255, 0, 0), 2)\n",
    "\t\tcv2.imshow('image', img)\n",
    "\tif event==cv2.EVENT_RBUTTONDOWN:\n",
    "\t\tprint(x, ' ', y)\n",
    "\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\t\tb = img[y, x, 0]\n",
    "\t\tg = img[y, x, 1]\n",
    "\t\tr = img[y, x, 2]\n",
    "\t\tcv2.putText(img, str(b) + ',' +\n",
    "\t\t\t\t\tstr(g) + ',' + str(r),\n",
    "\t\t\t\t\t(x,y), font, 1,\n",
    "\t\t\t\t\t(255, 255, 0), 2)\n",
    "\t\tcv2.imshow('image', img)\n",
    "\n",
    "#img = cv2.imread('Amani_capture_isp_1.png', 1)\n",
    "img = cv2.imread('Amani_capture_isp_2.png', 1)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "cv2.setMouseCallback('image', clickImage)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "\n",
    "**HomographyMatrix.py**\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "src_points = np.array([[1335,978],[1333,1928],[2989,1950],[2988,952]])\n",
    "dest_points = np.array([[299,856],[329,1948],[2131,1878],[2122,896]])\n",
    "\n",
    "h, status = cv2.findHomography(src_points, dest_points)\n",
    "\n",
    "im_src = cv2.imread('Amani_capture_isp_1.png')\n",
    "im_dst = cv2.imread('Amani_capture_isp_2.png')\n",
    "\n",
    "im_out = cv2.warpPerspective(im_src, h, (im_dst.shape[1],im_dst.shape[0]))\n",
    "print(h)\n",
    "cv2.imshow(\"Warped_Source_Image\", im_out)\n",
    "plt.imshow(im_out)\n",
    "plt.show()\n",
    "\n",
    "#HM\n",
    "#[[ 1.55385410e+00  4.56897321e-02 -1.76721065e+03]\n",
    "#[ 1.58042282e-01  1.38417516e+00 -5.57351990e+02]\n",
    "#[ 1.20560432e-04  1.62286723e-05  1.00000000e+00]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53807ea7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part II:\n",
    "\n",
    "Implement the image stitching application in MATLAB (not necessary to be real-time).\n",
    "\n",
    "Test your application for any FIVE of a set of 3 image-set available in the gsu_building_database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c4dbc",
   "metadata": {},
   "source": [
    "**Stitching.py:**\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "imagesPaths1 = [\"./GSU_Buildings/Set2/rialto1.jpeg\",\"./GSU_Buildings/Set2/rialto2.jpeg\",\"./GSU_Buildings/Set2/rialto3.jpeg\"]\n",
    "imagesPaths2 = [\"./GSU_Buildings/Set1/studentcentereast1.jpg\",\"./GSU_Buildings/Set1/studentcentereast2.jpg\",\"./GSU_Buildings/Set1/studentcentereast3.jpg\"]\n",
    "imagesPaths3 = [\"./GSU_Buildings/Set3/bookstore1.jpg\",\"./GSU_Buildings/Set3/bookstore2.jpg\",\"./GSU_Buildings/Set3/bookstore3.jpg\"]\n",
    "imagesPaths4 = [\"./GSU_Buildings/Set4/bookstore5.jpg\",\"./GSU_Buildings/Set4/bookstore6.jpg\",\"./GSU_Buildings/Set4/bookstore7.jpg\"]\n",
    "imagesPaths5 = [\"./GSU_Buildings/Set5/tdeck1.jpg\",\"./GSU_Buildings/Set5/tdeck2.jpg\",\"./GSU_Buildings/Set5/tdeck3.jpg\"]\n",
    "images = []\n",
    "\n",
    "for path in imagesPaths2:\n",
    "\timage = cv2.imread(path)\n",
    "\timages.append(image)\n",
    "\n",
    "stitcher = cv2.Stitcher_create()\n",
    "(status, stitched) = stitcher.stitch(images)\n",
    "print(stitched)\n",
    "if status == 0:\n",
    "\tcv2.imshow(\"Stitched images\", stitched)\n",
    "\tcv2.waitKey(0)\n",
    "else:\n",
    "\tprint(\"Failed to strich\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadc2fc",
   "metadata": {},
   "source": [
    "Images before stitching:\n",
    "\n",
    "![image1](Part-2/GSU_Buildings/Set1/studentcentereast1.jpg)\n",
    "![image2](Part-2/GSU_Buildings/Set1/studentcentereast2.jpg)\n",
    "![image3](Part-2/GSU_Buildings/Set1/studentcentereast3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2f764",
   "metadata": {},
   "source": [
    "Output of stitched images:\n",
    "\n",
    "![Output image](Part-2/stitched.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f29ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part III:\n",
    "\n",
    "Implement an application that will compute and display the INTEGRAL image feed along with the stereo and RGB feed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf2720",
   "metadata": {},
   "source": [
    "**IntegralImageFeed.py**\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import depthai as dai\n",
    "\n",
    "def integral_image(image, *, dtype=None):\n",
    "    if dtype is None and image.real.dtype.kind == 'f':\n",
    "        dtype = np.promote_types(image.dtype, np.float64)\n",
    "    S = image\n",
    "    for i in range(image.ndim):\n",
    "        S = S.cumsum(axis=i, dtype=dtype)\n",
    "    return S\n",
    "\n",
    "\n",
    "def integrate(ii, start, end):\n",
    "    start = np.atleast_2d(np.array(start))\n",
    "    end = np.atleast_2d(np.array(end))\n",
    "    rows = start.shape[0]\n",
    "    total_shape = ii.shape\n",
    "    total_shape = np.tile(total_shape, [rows, 1])\n",
    "    start_negatives = start < 0\n",
    "    end_negatives = end < 0\n",
    "    start = (start + total_shape) * start_negatives + \\\n",
    "             start * ~(start_negatives)\n",
    "    end = (end + total_shape) * end_negatives + \\\n",
    "           end * ~(end_negatives)\n",
    "\n",
    "    if np.any((end - start) < 0):\n",
    "        print('Error')\n",
    "    S = np.zeros(rows)\n",
    "    bit_perm = 2 ** ii.ndim\n",
    "    width = len(bin(bit_perm - 1)[2:])\n",
    "    for i in range(bit_perm):\n",
    "        binary = bin(i)[2:].zfill(width)\n",
    "        bool_mask = [bit == '1' for bit in binary]\n",
    "        sign = (-1)**sum(bool_mask)\n",
    "        bad = [np.any(((start[r] - 1) * bool_mask) < 0)\n",
    "               for r in range(rows)]\n",
    "        corner_points = (end * (np.invert(bool_mask))) + \\\n",
    "                         ((start - 1) * bool_mask)\n",
    "        S += [sign * ii[tuple(corner_points[r])] if(not bad[r]) else 0\n",
    "              for r in range(rows)]\n",
    "    return S\n",
    "\n",
    "streams = []\n",
    "streams.append('isp')\n",
    "@nb.njit(nb.uint16[::1] (nb.uint8[::1], nb.uint16[::1], nb.boolean), parallel=True, cache=True)\n",
    "def unpack_raw10(input, out, expand16bit):\n",
    "    lShift = 6 if expand16bit else 0\n",
    "    for i in np.arange(input.size // 5):\n",
    "        b4 = input[i * 5 + 4]\n",
    "        out[i * 4] = ((input[i * 5] << 2) | (b4 & 0x3)) << lShift\n",
    "        out[i * 4 + 1] = ((input[i * 5 + 1] << 2) | ((b4 >> 2) & 0x3)) << lShift\n",
    "        out[i * 4 + 2] = ((input[i * 5 + 2] << 2) | ((b4 >> 4) & 0x3)) << lShift\n",
    "        out[i * 4 + 3] = ((input[i * 5 + 3] << 2) | (b4 >> 6)) << lShift\n",
    "\n",
    "    return out\n",
    "\n",
    "pipeline = dai.Pipeline()\n",
    "cam = pipeline.createColorCamera()\n",
    "cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_12_MP)\n",
    "\n",
    "if 'isp' in streams:\n",
    "    xout_isp = pipeline.createXLinkOut()\n",
    "    xout_isp.setStreamName('isp')\n",
    "    cam.isp.link(xout_isp.input)\n",
    "\n",
    "device = dai.Device(pipeline)\n",
    "device.startPipeline()\n",
    "\n",
    "q_list = []\n",
    "for s in streams:\n",
    "    q = device.getOutputQueue(name=s, maxSize=3, blocking=True)\n",
    "    q_list.append(q)\n",
    "    cv2.namedWindow(s, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(s, (960, 540))\n",
    "\n",
    "capture_flag = False\n",
    "img_counter = 0\n",
    "while True:\n",
    "    for q in q_list:\n",
    "        name = q.getName()\n",
    "        data = q.get()\n",
    "        width, height = data.getWidth(), data.getHeight()\n",
    "        payload = data.getData()\n",
    "        capture_file_info_str = f\"Amani_capture_{name}_{img_counter}\"\n",
    "        if name == 'isp':\n",
    "            shape = (height * 3 // 2, width)\n",
    "            yuv420p = payload.reshape(shape).astype(np.uint8)\n",
    "            bgr = cv2.cvtColor(yuv420p, cv2.COLOR_YUV2BGR_IYUV)\n",
    "            frame = cv2.copyMakeBorder(bgr, 50, 50, 50, 50, cv2.BORDER_CONSTANT, (0,0,0))\n",
    "            frame = integral_image(frame)\n",
    "            frame = frame/np.amax(frame)\n",
    "            frame = np.clip(frame, 0,255)\n",
    "        if capture_flag:\n",
    "            filename = capture_file_info_str + '.png'\n",
    "            grayscale_img = np.ascontiguousarray(grayscale_img)\n",
    "            cv2.imwrite(filename, grayscale_img)\n",
    "        bgr = np.ascontiguousarray(bgr)\n",
    "        cv2.imshow(name, frame)\n",
    "    capture_flag = False\n",
    "    key = cv2.waitKey(5)\n",
    "    if key % 256 == 27:\n",
    "        break\n",
    "    elif key%256 == 32:\n",
    "        capture_flag = True\n",
    "        img_counter += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e54d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part IV:\n",
    "\n",
    "Implement the image stitching, for at least 1 pair of images. Use SIFT features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeab70a",
   "metadata": {},
   "source": [
    "Images before stitching:\n",
    "\n",
    "![Input image 1](Part-1/Amani_capture_isp_1.png)\n",
    "![Input image 2](Part-1/Amani_capture_isp_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1705a31",
   "metadata": {},
   "source": [
    "Image after stitching:\n",
    "\n",
    "\n",
    "![Output image](Part-4/output.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d7a71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part V:\n",
    "\n",
    "Repeat (4) using ORB features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fe578",
   "metadata": {},
   "source": [
    "**OrbImageStitching.py**\n",
    "```python\n",
    "import imutils\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "def getHomography(kps_train, kps_query, matches, reprojThresh):\n",
    "    kpsA = np.float32([kp.pt for kp in kps_train])\n",
    "    kpsB = np.float32([kp.pt for kp in kps_query])\n",
    "    if len(matches) > 4:\n",
    "        pointsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
    "        pointsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
    "        (Homography, status) = cv2.findHomography(pointsA, pointsB, cv2.RANSAC, reprojThresh)\n",
    "        print('matches: ', matches)\n",
    "        return (matches, Homography, status)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def GetImageKeyPoints(image):\n",
    "    descriptor = cv2.ORB_create()\n",
    "    kps, features = descriptor.detectAndCompute(image, None)\n",
    "    return (kps, features)\n",
    "\n",
    "def MatchImageKeyPoints(features_train, features_query, ratio):\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "    raw_match = bf.knnMatch(features_train, features_query, 2)\n",
    "    matches = []\n",
    "\n",
    "    for m, n in raw_match:\n",
    "        if m.distance < n.distance * ratio:\n",
    "            matches.append(m)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def TransformImageToGrayScale(result):\n",
    "    gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "    threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]\n",
    "    cnts = cv2.findContours(threshold.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    result = result[y:y + h, x:x + w]\n",
    "    return result\n",
    "\n",
    "\n",
    "img_train = cv2.imread(\"../Part-1/Amani_capture_isp_1.png\")\n",
    "img_query = cv2.imread(\"../Part-1/Amani_capture_isp_2.png\")\n",
    "gray_img_train = cv2.cvtColor(img_train, cv2.COLOR_RGB2GRAY)\n",
    "gray_img_query = cv2.cvtColor(img_query, cv2.COLOR_RGB2GRAY)\n",
    "kps_train, features_train = GetImageKeyPoints(gray_img_train)\n",
    "kps_query, features_query = GetImageKeyPoints(gray_img_query)\n",
    "print(features_train,features_query, kps_train, kps_query)\n",
    "matches = MatchImageKeyPoints(features_train, features_query, 0.75)\n",
    "print(matches)\n",
    "temp_img = cv2.drawMatches(img_train, kps_train, img_query, kps_query, np.random.choice(matches, 100), None,\n",
    "                           flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "M = getHomography(kps_train, kps_query, matches, 4)\n",
    "matches, homography, status = M\n",
    "width = img_train.shape[1] + img_query.shape[1]\n",
    "height = img_train.shape[0] + img_query.shape[0]\n",
    "imageResult = cv2.warpPerspective(img_train, homography, (width, height))\n",
    "imageResult[0:img_query.shape[0], 0:img_query.shape[1]] = img_query\n",
    "imageResult = TransformImageToGrayScale(imageResult)\n",
    "plt.imshow(imageResult)\n",
    "plt.show()\n",
    "cv2.imwrite(\"orboutput.png\", imageResult)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432ba4b",
   "metadata": {},
   "source": [
    "Image after stitching (same input images as above):\n",
    "\n",
    "\n",
    "![Output image](Part-5/orboutput.png)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "284e3458e5a53f6c9e70b531051d4666713a982ff7ff849372ee6bfb36aedb93"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('cv_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
