{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb487000",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f358249",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Github repo for assignment: https://github.com/brentonjackson/csc-4980/tree/master/Assignment3\n",
    "\n",
    "I'll be using Python for the assignments in this class, as opposed to Matlab.\n",
    "\n",
    "**Credit goes to Amani Hunter for assets and code**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa301a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part I:\n",
    "\n",
    "\n",
    "\n",
    "1. Capture 10s handheld video footage and pan from left to right or right to left.\n",
    "\n",
    "\n",
    "2. Pick any image frame from the 10 sec video footage.\n",
    "\n",
    "\n",
    "3. Pick a region of interest corresponding to an object in the image.\n",
    "\n",
    "\n",
    "4. Crop this region from the image.\n",
    "\n",
    "\n",
    "5. Then use this cropped region to compare with randomly picked 10 images in the dataset of 10 sec video frames, to see if there is a match for the object in the scenes from the 10 images. For comparison use sum of squared differences (SSD) or normalized correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca65ad",
   "metadata": {},
   "source": [
    "Below is the code for determining if there is a match for the object in the images and the output of SSD and correlation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54f3fa",
   "metadata": {},
   "source": [
    "**VideoRecorder.py** records the video footage and allows the user to choose specific frames by pressing SPACE bar\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import depthai as dai\n",
    "\n",
    "streams = []\n",
    "streams.append('isp')\n",
    "\n",
    "@nb.njit(nb.uint16[::1] (nb.uint8[::1], nb.uint16[::1], nb.boolean), parallel=True, cache=True)\n",
    "def unpack_raw10(input, out, expand16bit):\n",
    "    lShift = 6 if expand16bit else 0\n",
    "\n",
    "    for i in np.arange(input.size // 5):\n",
    "        b4 = input[i * 5 + 4]\n",
    "        out[i * 4]     = ((input[i * 5]     << 2) | ( b4       & 0x3)) << lShift\n",
    "        out[i * 4 + 1] = ((input[i * 5 + 1] << 2) | ((b4 >> 2) & 0x3)) << lShift\n",
    "        out[i * 4 + 2] = ((input[i * 5 + 2] << 2) | ((b4 >> 4) & 0x3)) << lShift\n",
    "        out[i * 4 + 3] = ((input[i * 5 + 3] << 2) |  (b4 >> 6)       ) << lShift\n",
    "\n",
    "    return out\n",
    "\n",
    "print(\"depthai version:\", dai.__version__)\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "cam = pipeline.createColorCamera()\n",
    "cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_12_MP)\n",
    "\n",
    "if 'isp' in streams:\n",
    "    xout_isp = pipeline.createXLinkOut()\n",
    "    xout_isp.setStreamName('isp')\n",
    "    cam.isp.link(xout_isp.input)\n",
    "\n",
    "device = dai.Device(pipeline)\n",
    "device.startPipeline()\n",
    "\n",
    "q_list = []\n",
    "for s in streams:\n",
    "    q = device.getOutputQueue(name=s, maxSize=3, blocking=True)\n",
    "    q_list.append(q)\n",
    "    cv2.namedWindow(s, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(s, (960, 540))\n",
    "\n",
    "capture_flag = False\n",
    "image_counter = 0\n",
    "while True:\n",
    "    for q in q_list:\n",
    "        name = q.getName()\n",
    "        data = q.get()\n",
    "        width, height = data.getWidth(), data.getHeight()\n",
    "        payload = data.getData()\n",
    "        capture_file_info_name = f\"frame{image_counter}\"\n",
    "        if name == 'isp':\n",
    "            shape = (height * 3 // 2, width)\n",
    "            yuv420p = payload.reshape(shape).astype(np.uint8)\n",
    "            bgr = cv2.cvtColor(yuv420p, cv2.COLOR_YUV2BGR_IYUV)\n",
    "            grayscale_img = cv2.cvtColor(bgr,cv2.COLOR_BGR2GRAY)\n",
    "        if capture_flag:\n",
    "            filename = capture_file_info_name + '.png'\n",
    "            print(filename)\n",
    "            grayscale_img = np.ascontiguousarray(grayscale_img)\n",
    "            cv2.imwrite(filename, grayscale_img)\n",
    "        bgr = np.ascontiguousarray(bgr)\n",
    "        cv2.imshow(name, grayscale_img)\n",
    "    capture_flag = False\n",
    "    key = cv2.waitKey(5)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key%256 == 32:\n",
    "        capture_flag = True\n",
    "        image_counter += 1\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71605d66",
   "metadata": {},
   "source": [
    "**Question1.py**:\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "frame0 = cv2.imread('frame10.png')\n",
    "frame0 = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
    "frame1 = cv2.imread('frame1.png')\n",
    "frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "frame2 = cv2.imread('frame2.png')\n",
    "frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "frame3 = cv2.imread('frame3.png')\n",
    "frame3 = cv2.cvtColor(frame3, cv2.COLOR_BGR2GRAY)\n",
    "frame4 = cv2.imread('frame4.png')\n",
    "frame4 = cv2.cvtColor(frame4, cv2.COLOR_BGR2GRAY)\n",
    "frame5 = cv2.imread('frame5.png')\n",
    "frame5 = cv2.cvtColor(frame5, cv2.COLOR_BGR2GRAY)\n",
    "frame6 = cv2.imread('frame6.png')\n",
    "frame6 = cv2.cvtColor(frame6, cv2.COLOR_BGR2GRAY)\n",
    "frame7 = cv2.imread('frame7.png')\n",
    "frame7 = cv2.cvtColor(frame7, cv2.COLOR_BGR2GRAY)\n",
    "frame8 = cv2.imread('frame8.png')\n",
    "frame8 = cv2.cvtColor(frame8, cv2.COLOR_BGR2GRAY)\n",
    "frame9 = cv2.imread('frame9.png')\n",
    "frame9 = cv2.cvtColor(frame9, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "frame0 = cv2.resize(frame0, (87, 87))\n",
    "\n",
    "frame1 = cv2.resize(frame1, (87, 87))\n",
    "\n",
    "frame2 = cv2.resize(frame2, (87, 87))\n",
    "\n",
    "frame3 = cv2.resize(frame3, (87, 87))\n",
    "\n",
    "frame4 = cv2.resize(frame4, (87, 87))\n",
    "\n",
    "frame5 = cv2.resize(frame5, (87, 87))\n",
    "\n",
    "frame6 = cv2.resize(frame6, (87, 87))\n",
    "\n",
    "frame7 = cv2.resize(frame7, (87, 87))\n",
    "\n",
    "frame8 = cv2.resize(frame8, (87, 87))\n",
    "\n",
    "frame9 = cv2.resize(frame9, (87, 87))\n",
    "\n",
    "ssd0 = []\n",
    "\n",
    "ssd01 = np.sum((np.square(frame0 - frame1)))\n",
    "\n",
    "ssd0.append(ssd01)\n",
    "\n",
    "ssd02 = np.sum((np.square(frame0 - frame2)))\n",
    "\n",
    "ssd0.append(ssd02)\n",
    "\n",
    "ssd03 = np.sum((np.square(frame0 - frame3)))\n",
    "\n",
    "ssd0.append(ssd03)\n",
    "\n",
    "ssd04 = np.sum((np.square(frame0 - frame4)))\n",
    "\n",
    "ssd0.append(ssd04)\n",
    "\n",
    "ssd05 = np.sum((np.square(frame0 - frame5)))\n",
    "\n",
    "ssd0.append(ssd05)\n",
    "\n",
    "ssd06 = np.sum((np.square(frame0 - frame6)))\n",
    "\n",
    "ssd0.append(ssd06)\n",
    "\n",
    "ssd07 = np.sum((np.square(frame0 - frame7)))\n",
    "\n",
    "ssd0.append(ssd07)\n",
    "\n",
    "ssd08 = np.sum((np.square(frame0 - frame8)))\n",
    "\n",
    "ssd0.append(ssd08)\n",
    "\n",
    "ssd09 = np.sum((np.square(frame0 - frame9)))\n",
    "\n",
    "ssd0.append(ssd09)\n",
    "\n",
    "print('SSD: ',ssd0)\n",
    "corr0 = []\n",
    "\n",
    "def correlation_coefficient(patch1, patch2):\n",
    "    product = np.mean((patch1 - patch1.mean()) * (patch2 - patch2.mean()))\n",
    "    stds = patch1.std() * patch2.std()\n",
    "    if stds == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        product /= stds\n",
    "        return product\n",
    "\n",
    "corr01 = correlation_coefficient(frame0, frame1)\n",
    "\n",
    "corr0.append(corr01)\n",
    "\n",
    "corr02 = correlation_coefficient(frame0, frame2)\n",
    "\n",
    "corr0.append(corr02)\n",
    "\n",
    "corr03 = correlation_coefficient(frame0, frame3)\n",
    "\n",
    "corr0.append(corr03)\n",
    "\n",
    "corr04 = correlation_coefficient(frame0, frame4)\n",
    "\n",
    "corr0.append(corr04)\n",
    "\n",
    "corr05 = correlation_coefficient(frame0, frame5)\n",
    "\n",
    "corr0.append(corr05)\n",
    "\n",
    "corr06 = correlation_coefficient(frame0, frame6)\n",
    "\n",
    "corr0.append(corr06)\n",
    "\n",
    "corr07 = correlation_coefficient(frame0, frame7)\n",
    "\n",
    "corr0.append(corr07)\n",
    "\n",
    "corr08 = correlation_coefficient(frame0, frame8)\n",
    "\n",
    "corr0.append(corr08)\n",
    "\n",
    "corr09 = correlation_coefficient(frame0, frame9)\n",
    "\n",
    "corr0.append(corr09)\n",
    "\n",
    "print('Correlation: ',corr0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b8a1b",
   "metadata": {},
   "source": [
    "**Output result:**\n",
    "\n",
    "SSD:  [790104, 796043, 795369, 813209, 819670, 810515, 804582, 806397, 809074]\n",
    "\n",
    "\n",
    "Correlation:  [-0.09805182753864389, -0.043593190599718695, -0.03368530968969049, -0.01546628121144833, 0.16660657838017948, 0.1741383019089876, 0.16875910526978377, 0.20127983457759524, 0.3952648668796892]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53807ea7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part II:\n",
    "\n",
    "Implement the motion tracking equation from fundamental principles.\n",
    "\n",
    "Select any 2 consecutive frames from the set from problem 1 and compute the motion function estimates.\n",
    "\n",
    "Conduct image registration to realign the frames.\n",
    "\n",
    "Repeat test for all consecutive pairs of frames in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c4dbc",
   "metadata": {},
   "source": [
    "**Question2.py:**\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def motion_constraints(Iref, Inext):\n",
    "    kernel_x = np.array([[-1., 1.], [-1., 1.]])*.25\n",
    "    kernel_y = np.array([[-1., -1.], [1., 1.]])*.25\n",
    "    kernel_t = np.array([[1., 1.], [1., 1.]])*.25\n",
    "    Iref = Iref / 255.\n",
    "    Inext = Inext / 255.\n",
    "    mode = 'same'\n",
    "\n",
    "    Ix = signal.convolve2d(Iref, kernel_x, boundary='symm', mode=mode)\n",
    "    Iy = signal.convolve2d(Iref, kernel_y, boundary='symm', mode=mode)\n",
    "    It = signal.convolve2d(Inext, kernel_t, boundary='symm', mode=mode) + signal.convolve2d(Iref, -kernel_t, boundary='symm', mode=mode)\n",
    "    return Ix, Iy, It\n",
    "frameA = \"../Part-1/frame1.png\"\n",
    "frameB = \"../Part-1/frame2.png\"\n",
    "frame0 = cv2.imread(frameA)\n",
    "frame0 = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
    "frame0 = cv2.resize(frame0, (87,87))\n",
    "frame1 = cv2.imread(frameB)\n",
    "frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "frame1 = cv2.resize(frame1, (87,87))\n",
    "\n",
    "Ix, Iy, It = motion_constraints(frame0, frame1)\n",
    "print(Ix, Iy, It)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f29ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part III:\n",
    "\n",
    "For the video (problem 1) you have taken, plot the optical flow vectors on each frame.\n",
    "\n",
    "   - (i) treating every\n",
    "     previous frame as a reference frame\n",
    "   - (ii) treating every 11th frame as a reference frame\n",
    "   - (iii) treating every 31st frame as a reference frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf2720",
   "metadata": {},
   "source": [
    "**OpticalFlow.py** plots the optical flow vectors on each frame\n",
    "```python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "feature_params = dict(maxCorners = 300, qualityLevel = 0.2, minDistance = 2, blockSize = 7)\n",
    "\n",
    "lk_params = dict(winSize = (15,15), maxLevel = 2, criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "cap = cv.VideoCapture(\"shibuya.mp4\")\n",
    "# cap = cv.VideoCapture(\"output.mp4\")\n",
    "\n",
    "color = (0, 255, 0)\n",
    "red = (255,0,0)\n",
    "\n",
    "ret, first_frame = cap.read()\n",
    "# Converts frame to grayscale because we only need the luminance channel for detecting edges - less computationally expensive\n",
    "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "prev = cv.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
    "\n",
    "mask = np.zeros_like(first_frame)\n",
    "\n",
    "while(cap.isOpened()):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    prev = cv.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
    "    next, status, error = cv.calcOpticalFlowPyrLK(prev_gray, gray, prev, None, **lk_params)\n",
    "\n",
    "    good_old = prev[status == 1].astype(int)\n",
    "\n",
    "    good_new = next[status == 1].astype(int)\n",
    "\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "\n",
    "        a, b = new.ravel()\n",
    "\n",
    "        c, d = old.ravel()\n",
    "\n",
    "        mask = cv.line(mask, (a, b), (c, d), color, 2)\n",
    "\n",
    "        frame = cv.circle(frame, (a, b), 3, red, -1)\n",
    "\n",
    "    output = cv.add(frame, mask)\n",
    "\n",
    "    prev_gray = gray.copy()\n",
    "\n",
    "    prev = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    cv.imshow(\"sparse optical flow\", output)\n",
    "\n",
    "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e54d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part IV:\n",
    "\n",
    "Implement a feature based object detection application (from scratch) for detecting an object of\n",
    "   your choice.\n",
    "\n",
    "   - Test it for at least 2 differently looking objects.\n",
    "\n",
    "   - Validate your results by testing against\n",
    "     built-in object detection functions/code in MATLAB/OpenCV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeab70a",
   "metadata": {},
   "source": [
    "**FeatureDetector.py** detects a user's hand\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Function to find angle between two vectors\n",
    "def Angle(v1, v2):\n",
    "    dot = np.dot(v1, v2)\n",
    "    x_modulus = np.sqrt((v1 * v1).sum())\n",
    "    y_modulus = np.sqrt((v2 * v2).sum())\n",
    "    cos_angle = dot / x_modulus / y_modulus\n",
    "    angle = np.degrees(np.arccos(cos_angle))\n",
    "    return angle\n",
    "\n",
    "\n",
    "# Function to find distance between two points in a list of lists\n",
    "def FindDistance(A, B):\n",
    "    return np.sqrt(np.power((A[0][0] - B[0][0]), 2) + np.power((A[0][1] - B[0][1]), 2))\n",
    "\n",
    "# Creating a window for HSV track bars\n",
    "cv2.namedWindow('HSV_TrackBar')\n",
    "\n",
    "# Starting with 100's to prevent error while masking\n",
    "h, s, v = 100, 100, 100\n",
    "\n",
    "# Creating track bar\n",
    "cv2.createTrackbar('h', 'HSV_TrackBar', 0, 179, nothing)\n",
    "cv2.createTrackbar('s', 'HSV_TrackBar', 0, 255, nothing)\n",
    "cv2.createTrackbar('v', 'HSV_TrackBar', 0, 255, nothing)\n",
    "\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "xoutVideo = pipeline.create(dai.node.XLinkOut)\n",
    "\n",
    "xoutVideo.setStreamName(\"feature detector\")\n",
    "\n",
    "camRgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "camRgb.setVideoSize(1000, 1000)\n",
    "\n",
    "xoutVideo.input.setBlocking(False)\n",
    "xoutVideo.input.setQueueSize(1)\n",
    "\n",
    "camRgb.video.link(xoutVideo.input)\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "    video = device.getOutputQueue(name=\"feature detector\", maxSize=1, blocking=False)\n",
    "    while True:\n",
    "        videoIn = video.get()\n",
    "        frame = videoIn.getCvFrame()\n",
    "        start_time = time.time()\n",
    "        blur = cv2.blur(frame, (3, 3))\n",
    "\n",
    "        # Convert to HSV color space\n",
    "        hsv = cv2.cvtColor(blur, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Create a binary image with where white will be skin colors and rest is black\n",
    "        mask2 = cv2.inRange(hsv, np.array([2, 50, 50]), np.array([15, 255, 255]))\n",
    "\n",
    "        # Kernel matrices for morphological transformation\n",
    "        kernel_square = np.ones((11, 11), np.uint8)\n",
    "        kernel_ellipse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "        # Perform morphological transformations to filter out the background noise\n",
    "        # Dilation increase skin color area\n",
    "        # Erosion increase skin color area\n",
    "        dilation = cv2.dilate(mask2, kernel_ellipse, iterations=1)\n",
    "        erosion = cv2.erode(dilation, kernel_square, iterations=1)\n",
    "        dilation2 = cv2.dilate(erosion, kernel_ellipse, iterations=1)\n",
    "        filtered = cv2.medianBlur(dilation2, 5)\n",
    "        kernel_ellipse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 8))\n",
    "        dilation2 = cv2.dilate(filtered, kernel_ellipse, iterations=1)\n",
    "        kernel_ellipse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        dilation3 = cv2.dilate(filtered, kernel_ellipse, iterations=1)\n",
    "        median = cv2.medianBlur(dilation2, 5)\n",
    "        ret, thresh = cv2.threshold(median, 127, 255, 0)\n",
    "\n",
    "        # Find contours of the filtered frame\n",
    "        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Draw Contours\n",
    "        # cv2.drawContours(frame, cnt, -1, (122,122,0), 3)\n",
    "        # cv2.imshow('Dilation',median)\n",
    "\n",
    "        # Find Max contour area (Assume that hand is in the frame)\n",
    "        max_area = 100\n",
    "        ci = 0\n",
    "        for i in range(len(contours)):\n",
    "            cnt = contours[i]\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if (area > max_area):\n",
    "                max_area = area\n",
    "                ci = i\n",
    "\n",
    "            # Largest area contour\n",
    "        if len(contours) > 0:\n",
    "            cnts = contours[ci]\n",
    "\n",
    "        # Find convex hull\n",
    "            hull = cv2.convexHull(cnts)\n",
    "\n",
    "        # Find convex defects\n",
    "            hull2 = cv2.convexHull(cnts, returnPoints=False)\n",
    "            defects = cv2.convexityDefects(cnts, hull2)\n",
    "\n",
    "        # Get defect points and draw them in the original image\n",
    "            FarDefect = []\n",
    "            for i in range(defects.shape[0]):\n",
    "                s, e, f, d = defects[i, 0]\n",
    "                start = tuple(cnts[s][0])\n",
    "                end = tuple(cnts[e][0])\n",
    "                far = tuple(cnts[f][0])\n",
    "                FarDefect.append(far)\n",
    "                cv2.line(frame, start, end, [0, 255, 0], 1)\n",
    "                cv2.circle(frame, far, 10, [100, 255, 255], 3)\n",
    "\n",
    "        # Find moments of the largest contour\n",
    "            moments = cv2.moments(cnts)\n",
    "\n",
    "        # Central mass of first order moments\n",
    "            if moments['m00'] != 0:\n",
    "                cx = int(moments['m10'] / moments['m00'])  # cx = M10/M00\n",
    "                cy = int(moments['m01'] / moments['m00'])  # cy = M01/M00\n",
    "            centerMass = (cx, cy)\n",
    "\n",
    "        # Draw center mass\n",
    "            cv2.circle(frame, centerMass, 7, [100, 0, 255], 2)\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(frame, 'Center of Hand', tuple(centerMass), font, 2, (255, 255, 255), 2)\n",
    "\n",
    "        # Distance from each finger defect(finger webbing) to the center mass\n",
    "            distanceBetweenDefectsToCenter = []\n",
    "            for i in range(0, len(FarDefect)):\n",
    "                x = np.array(FarDefect[i])\n",
    "                centerMass = np.array(centerMass)\n",
    "                distance = np.sqrt(np.power(x[0] - centerMass[0], 2) + np.power(x[1] - centerMass[1], 2))\n",
    "                distanceBetweenDefectsToCenter.append(distance)\n",
    "\n",
    "        # Get an average of three shortest distances from finger webbing to center mass\n",
    "            sortedDefectsDistances = sorted(distanceBetweenDefectsToCenter)\n",
    "            AverageDefectDistance = np.mean(sortedDefectsDistances[0:2])\n",
    "\n",
    "        # Get fingertip points from contour hull\n",
    "        # If points are in proximity of 80 pixels, consider as a single point in the group\n",
    "            finger = []\n",
    "            for i in range(0, len(hull) - 1):\n",
    "                if (np.absolute(hull[i][0][0] - hull[i + 1][0][0]) > 80) or (\n",
    "                        np.absolute(hull[i][0][1] - hull[i + 1][0][1]) > 80):\n",
    "                    if hull[i][0][1] < 500:\n",
    "                        finger.append(hull[i][0])\n",
    "\n",
    "        # The fingertip points are 5 hull points with largest y coordinates\n",
    "            finger = sorted(finger, key=lambda x: x[1])\n",
    "            fingers = finger[0:5]\n",
    "            print('Fingers: ', fingers)\n",
    "\n",
    "        # Calculate distance of each finger tip to the center mass\n",
    "            fingerDistance = []\n",
    "            for i in range(0, len(fingers)):\n",
    "                distance = np.sqrt(np.power(fingers[i][0] - centerMass[0], 2) + np.power(fingers[i][1] - centerMass[0], 2))\n",
    "                fingerDistance.append(distance)\n",
    "\n",
    "        # Finger is pointed/raised if the distance of between fingertip to the center mass is larger\n",
    "        # than the distance of average finger webbing to center mass by 130 pixels\n",
    "            result = 0\n",
    "            for i in range(0, len(fingers)):\n",
    "                if fingerDistance[i] > AverageDefectDistance + 130:\n",
    "                    result = result + 1\n",
    "\n",
    "        # Print number of pointed fingers\n",
    "            cv2.putText(frame, str(result), (100, 100), font, 2, (255, 255, 255), 2)\n",
    "\n",
    "        # show height raised fingers\n",
    "            #cv2.putText(frame,'finger1',tuple(finger[0]),font,2,(255,255,255),2)\n",
    "            #cv2.putText(frame,'finger2',tuple(finger[1]),font,2,(255,255,255),2)\n",
    "            #cv2.putText(frame,'finger3',tuple(finger[2]),font,2,(255,255,255),2)\n",
    "            #cv2.putText(frame,'finger4',tuple(finger[3]),font,2,(255,255,255),2)\n",
    "            #cv2.putText(frame,'finger5',tuple(finger[4]),font,2,(255,255,255),2)\n",
    "            #cv2.putText(frame,'finger6',tuple(finger[5]),font,2,(255,255,255),2)\n",
    "            #cv2.putText(frame,'finger7',tuple(finger[6]),font,2,(255,255,255),2)\n",
    "            #cv2.putText(frame,'finger8',tuple(finger[7]),font,2,(255,255,255),2)\n",
    "\n",
    "        # Print bounding rectangle\n",
    "            x, y, w, h = cv2.boundingRect(cnts)\n",
    "            img = cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            cv2.drawContours(frame, [hull], -1, (255, 255, 255), 2)\n",
    "\n",
    "        ##### Show final image ########\n",
    "            cv2.imshow('Dilation', frame)\n",
    "        ###############################\n",
    "\n",
    "        # Print execution time\n",
    "        # print time.time()-start_time\n",
    "\n",
    "        # close the output video by pressing 'ESC'\n",
    "            k = cv2.waitKey(5) & 0xFF\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                break\n",
    "\n",
    "        #cv2.imshow('Face Detector', frame)\n",
    "      #  if cv2.waitKey(1) == ord('q'):\n",
    "           # break\n",
    "cv2.destroyAllWindows()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d7a71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part V:\n",
    "\n",
    "Implement a real-time face tracking application that will detect as many faces there are with a\n",
    "   scene, and identify the person’s facial region (draw a bounding box) whose is sought for by the user\n",
    "   (you must ask for a person in your application and it should show a bounding box over the person\n",
    "   of interest).\n",
    "\n",
    "   - Validate at least 20 times and present the recognition performance metrics (accuracy,\n",
    "     precision, recall and Intersection over Union (IoU))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fe578",
   "metadata": {},
   "source": [
    "**FaceTracking.py** detects faces from the known faces given to it in real-time.\n",
    "```python\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "xoutVideo = pipeline.create(dai.node.XLinkOut)\n",
    "\n",
    "xoutVideo.setStreamName(\"face detector\")\n",
    "\n",
    "camRgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "camRgb.setVideoSize(860, 720)\n",
    "\n",
    "xoutVideo.input.setBlocking(False)\n",
    "xoutVideo.input.setQueueSize(1)\n",
    "\n",
    "camRgb.video.link(xoutVideo.input)\n",
    "\n",
    "amani_image = face_recognition.load_image_file(\"Amani_pic.jpeg\")\n",
    "amani_face_encoding = face_recognition.face_encodings(amani_image)[0]\n",
    "brent_image = face_recognition.load_image_file(\"BJackson.jpg\")\n",
    "brent_face_encoding = face_recognition.face_encodings(brent_image)[0]\n",
    "\n",
    "known_face_encodings = [\n",
    "    amani_face_encoding,\n",
    "    brent_face_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Amani Hunter\",\n",
    "    \"Brent\"\n",
    "]\n",
    "val = input(\"Enter name of person to detect: \")\n",
    "print(val)\n",
    "\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "top_lip = []\n",
    "bottom_lip = []\n",
    "center_points = []\n",
    "process_this_frame = True\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "    video = device.getOutputQueue(name=\"face detector\", maxSize=1, blocking=False)\n",
    "\n",
    "    while True:\n",
    "        videoIn = video.get()\n",
    "        frame = videoIn.getCvFrame()\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        if process_this_frame:\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "            face_landmarks_list = face_recognition.face_landmarks(rgb_small_frame)\n",
    "            face_names = []\n",
    "            for index, face_encoding in enumerate(face_encodings):\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                name = \"Unknown Individual\"\n",
    "\n",
    "                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                if name == 'Amani Hunter' or name == 'Brent' and val == name:\n",
    "                    keys = list(face_landmarks_list[index].keys())\n",
    "                    top_lip = face_landmarks_list[index][keys[-2]]\n",
    "                    bottom_lip = face_landmarks_list[index][keys[-1]]\n",
    "                    top_lip = np.array(top_lip, dtype=np.int32)\n",
    "                    bottom_lip = np.array(bottom_lip, dtype=np.int32)\n",
    "                    top_lip = top_lip * 4\n",
    "                    bottom_lip = bottom_lip * 4\n",
    "                    center_top_lip = np.mean(top_lip, axis=0)\n",
    "                    center_top_lip = center_top_lip.astype('int')\n",
    "                    center_points.append(center_top_lip)\n",
    "                face_names.append(name)\n",
    "        process_this_frame = not process_this_frame\n",
    "\n",
    "       # cv2.polylines(frame, np.array([top_lip]), 1, (255, 255, 255))\n",
    "        #cv2.polylines(frame, np.array([bottom_lip]), 1, (255, 255, 255))\n",
    "        for i in range(1, len(center_points)):\n",
    "            if center_points[i - 1] is None or center_points[i] is None:\n",
    "                continue\n",
    "           # cv2.line(frame, tuple(center_points[i - 1]), tuple(center_points[i]), (0, 0, 255), 2)\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "        cv2.imshow('Face Detector', frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45286311",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part VI:\n",
    "\n",
    "Fix a marker on a wall or a flat vertical surface.\n",
    "\n",
    "   - From a distance D, keeping the camera stationed static\n",
    "     (not handheld and mounted on a tripod or placed on a flat surface), capture an image such that the marker\n",
    "     is registered.\n",
    "\n",
    "   - Then translate the camera by T units along the axis parallel to the ground (horizontal) and\n",
    "     then capture another image, with the marker being registered.\n",
    "\n",
    "   - Compute D using disparity based depth\n",
    "     estimation in stereo-vision theory.\n",
    "\n",
    "   (Note: you can pick any value for D and T. Keep in mind that T cannot\n",
    "   be large as the marker may get out of view. Of course this depends on D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507bc97",
   "metadata": {},
   "source": [
    "Images captured:\n",
    "\n",
    "![Input image](Part-6/frame1.png)\n",
    "![Input image](Part-6/frame2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa3c13",
   "metadata": {},
   "source": [
    "**Question6.py** computes the disparity between these two images:\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "aruco = cv2.aruco\n",
    "\n",
    "def computeDisparity(img, markerSize =4, totalMarkers=50, draw=True):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    key = getattr(aruco, f'DICT_{markerSize}X{markerSize}_{totalMarkers}')\n",
    "    arucoDict = aruco.Dictionary_get(key)\n",
    "    arucoParam = aruco.DetectorParameters_create()\n",
    "    bboxs, ids, rejected = aruco.detectMarkers(gray, arucoDict, parameters = arucoParam)\n",
    "    return (bboxs)\n",
    "\n",
    "img0 = cv2.imread('frame1.png')\n",
    "img1 = cv2.imread('frame2.png')\n",
    "\n",
    "baseline = 11.5\n",
    "focal_length = 1.636331765375964e+03\n",
    "bbox1= computeDisparity(img0)\n",
    "bbox2 = computeDisparity(img1)\n",
    "\n",
    "d = (baseline * focal_length)/(bbox1[0][0][3][0]-bbox2[0][0][3][0])\n",
    "print('Disparity: ', d)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e18c7d",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "\n",
    "Disparity:  19.561138567384184"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
